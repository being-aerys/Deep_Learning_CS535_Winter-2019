{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-7-2d0dbef09b2a>, line 102)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-7-2d0dbef09b2a>\"\u001b[1;36m, line \u001b[1;32m102\u001b[0m\n\u001b[1;33m    print()\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Aashish Adhikari\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "try:\n",
    "   import _pickle as pickle\n",
    "except:\n",
    "   import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# This is a class for a LinearTransform layer which takes an input \n",
    "# weight matrix W and computes W x as the forward step\n",
    "\n",
    "\n",
    "\n",
    "class LinearTransform(object):\n",
    "\n",
    "    def __init__(self, W, b):\n",
    "        self.weights = W\n",
    "        self.bias = b\n",
    "    def forward(self, x):\n",
    "        print()\n",
    "    def backward(\n",
    "        self, \n",
    "        grad_output, \n",
    "        learning_rate=0.0, \n",
    "        momentum=0.0, \n",
    "        l2_penalty=0.0,\n",
    "    ):\n",
    "        print()\n",
    "\t# DEFINE backward function\n",
    "# ADD other operations in LinearTransform if needed\n",
    "\n",
    "\n",
    "\n",
    "# This is a class for a ReLU layer max(x,0)\n",
    "\n",
    "\n",
    "\n",
    "class ReLU(object):\n",
    "\n",
    "    def forward(self, x):\n",
    "\t# DEFINE forward function\n",
    "        print()\n",
    "    def backward(\n",
    "        self, \n",
    "        grad_output, \n",
    "        learning_rate=0.0, \n",
    "        momentum=0.0, \n",
    "        l2_penalty=0.0,\n",
    "    ):\n",
    "        print()\n",
    "    # DEFINE backward function\n",
    "# ADD other operations in ReLU if needed\n",
    "\n",
    "\n",
    "\n",
    "# This is a class for a sigmoid layer followed by a cross entropy layer, the reason \n",
    "# this is put into a single layer is because it has a simple gradient form\n",
    "\n",
    "\n",
    "\n",
    "class SigmoidCrossEntropy(object):\n",
    "    def forward(self, x):\n",
    "        print()\n",
    "        \n",
    "    def backward(self,grad_output,learning_rate=0.0,momentum=0.0,l2_penalty=0.0):\n",
    "        print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# This is a class for the Multilayer perceptron\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MLP(object):\n",
    "\n",
    "    def __init__(self, input_dims, hidden_units):\n",
    "    # INSERT CODE for initializing the network\n",
    "#         self.input_dims = input_dims\n",
    "#         self.hidden_units = hidden_units\n",
    "        self.first_Layer_Weights = np.random.rand(input_dims, hidden_units)\n",
    "        print(self.first_Layer_Weights.shape)\n",
    "\n",
    "    def train(self,x_batch,y_batch,learning_rate,momentum,l2_penalty):\n",
    "        \n",
    "        for sample,label in (x_batch, y_batch):\n",
    "            #yeha bata linear transform ma pathai\n",
    "            \n",
    "            \n",
    "        #one by one each example lai agadi pathaune 10 ota palai palo ayepaxi tiotal cumulative ma kaam garne yeha\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        ######Yeha k garne bhar aaba\n",
    "        \n",
    "        \n",
    "\t# INSERT CODE for training the network\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def evaluate(self, x, y):\n",
    "        print()\n",
    "\t# INSERT CODE for testing the network\n",
    "# ADD other operations and data entries in MLP if needed\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if sys.version_info[0] < 3:\n",
    "        print(\"system version is less than 3\")\n",
    "        #train_x, train_y, test_x, test_y = \n",
    "        data = pickle.load(open('dataset_folder/cifar_2class_py2.p', 'rb'))\n",
    "        \n",
    "    else:\n",
    "\t    #train_x, train_y, test_x, test_y \n",
    "        data = pickle.load(open('dataset_folder/cifar_2class_py2.p', 'rb'), encoding='bytes')\n",
    "    #print(data)\n",
    "    \n",
    "    #print(data[b'test_data'])\n",
    "    train_x = data[b'train_data']\n",
    "    train_y = data[b'train_labels']\n",
    "    test_x = data[b'test_data']\n",
    "    test_y = data[b'test_labels']\n",
    "    \n",
    "#     print(test_y)\n",
    "\t\n",
    "    num_examples, input_dims = train_x.shape\n",
    "\t# INSERT YOUR CODE HERE\n",
    "    #ask the user about the the number of hidden nodes hs/ she wants\n",
    "    \n",
    "    #inp = input(\"How many hidden nodes do you want?\\n\")\n",
    "    #num_of_hidden_nodes = int(inp)\n",
    "    num_of_hidden_nodes = 2\n",
    "    print(\"dimension of each example is \",input_dims)\n",
    "    \n",
    "    \n",
    "\t\n",
    "    \n",
    "    \n",
    "    # YOU CAN CHANGE num_epochs AND num_batches TO YOUR DESIRED VALUES\n",
    "    num_epochs = 10\n",
    "    num_batches = 1000\n",
    "    learning_rate = [0.001,0.003,0.01,0.03,0.1,0.3,1,3,10]\n",
    "    inertia_of_momentum = [0.001,0.003,0.01,0.03,0.1,0.3,1,3,10]\n",
    "    l2_penalty_factor = [0.0000001,0.0000003,0.000001,0.000003,0.00001,0.00003,0.0001,0.0003,0.001,0.003,0.01,0.03,1,3,10]\n",
    "    print(\"Choose the corresponding index number for the learning rate you want to use\")\n",
    "    lr = int(input(\"/[0.001,0.003,0.01,0.03,0.1,0.3,1,3,10/]\"))\n",
    "    print(\"Choose the corresponding index number for the inertia of momentum you want to use\")\n",
    "    iner = int(input(\"/[0.001,0.003,0.01,0.03,0.1,0.3,1,3,10/]\"))\n",
    "    print(\"Choose the corresponding index number for the L2 penalty factor you want to use\")\n",
    "    penalty = int(input(\"/[0.0000001,0.0000003,0.000001,0.000003,0.00001,0.00003,0.0001,0.0003,0.001,0.003,0.01,0.03,1,3,10/]\"))\n",
    "    mlp = MLP(input_dims, num_of_hidden_nodes)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "\t# INSERT YOUR CODE FOR EACH EPOCH HERE\n",
    "\n",
    "        for b in range(num_batches):\n",
    "            total_loss = 0.0\n",
    "            print(\"num of examples num of batches \",num_examples, \" \",num_batches)\n",
    "            \n",
    "            batch_start = int( (num_examples / num_batches) * b)\n",
    "            batch_end = int((num_examples / num_batches)*(b +1) - 1)\n",
    "            print(\"batch start and end\",batch_start,\" \",batch_end)\n",
    "            print(\"Chosen learning rate, inertia of momentum and l2 penalty factor are\")\n",
    "            print(learning_rate[lr], inertia_of_momentum[iner], l2_penalty_factor[penalty])\n",
    "            loss = mlp.train(train_x[batch_start:batch_end,...],train_y[batch_start:batch_end,...],int(learning_rate[lr]), inertia_of_momentum[iner], l2_penalty_factor[penalty])\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            ####mlp lai cll garda aba kata kata k hunxa network ma layer haru ma kaam gar ani tespaxi directly tesko loss return garne \n",
    "            #####actual loss function ma k garne \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # INSERT YOUR CODE FOR EACH MINI_BATCH HERE\n",
    "\t\t\t# MAKE SURE TO UPDATE total_loss\n",
    "            print(\n",
    "                '\\r[Epoch {}, mb {}]    Avg.Loss = {:.3f}'.format(\n",
    "                    epoch + 1,\n",
    "                    b + 1,\n",
    "                    total_loss,\n",
    "                ),\n",
    "                end='',\n",
    "            )\n",
    "            \n",
    "            #after each mini bach update you want to update the momentum value\n",
    "            sys.stdout.flush()\n",
    "\t\t# INSERT YOUR CODE AFTER ALL MINI_BATCHES HERE\n",
    "\t\t# MAKE SURE TO COMPUTE train_loss, train_accuracy, test_loss, test_accuracy\n",
    "        print()\n",
    "        print('    Train Loss: {:.3f}    Train Acc.: {:.2f}%'.format(\n",
    "            train_loss,\n",
    "            100. * train_accuracy,\n",
    "        ))\n",
    "        print('    Test Loss:  {:.3f}    Test Acc.:  {:.2f}%'.format(\n",
    "            test_loss,\n",
    "            100. * test_accuracy,\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
