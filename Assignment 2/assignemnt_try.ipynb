{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose the corresponding index number for the learning rate you want to use\n",
      "Choose the corresponding index number for the inertia of momentum you want to use\n",
      "Choose the corresponding index number for the L2 penalty factor you want to use\n",
      "\n",
      "\n",
      " Epoch is  1\n",
      "train size is   (10000, 1000)\n",
      "Z 1 yeha   [[53645.72151255 55544.42544806]\n",
      " [74770.45538761 75080.07170283]\n",
      " [66554.01199422 67906.6393842 ]\n",
      " [71502.75392858 72638.96225871]\n",
      " [96437.09728536 98387.20110925]\n",
      " [46793.56310548 47708.25102555]\n",
      " [61741.11696527 62451.53535632]\n",
      " [43511.52284822 44952.48564216]\n",
      " [70418.55095702 72060.35338244]\n",
      " [48027.46453977 50122.50942569]]\n",
      "y_cap or sigmoid_output [[0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]]\n",
      " y y cap Z1 loss  [[1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]] [[0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]\n",
      " [0.99999999]] [[53645.72151255 55544.42544806]\n",
      " [74770.45538761 75080.07170283]\n",
      " [66554.01199422 67906.6393842 ]\n",
      " [71502.75392858 72638.96225871]\n",
      " [96437.09728536 98387.20110925]\n",
      " [46793.56310548 47708.25102555]\n",
      " [61741.11696527 62451.53535632]\n",
      " [43511.52284822 44952.48564216]\n",
      " [70418.55095702 72060.35338244]\n",
      " [48027.46453977 50122.50942569]] [[110.52408447]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'grad_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-cca7108ad7c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    458\u001b[0m             \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_for_this_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_start\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_end\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m             \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_for_this_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_start\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_end\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 460\u001b[1;33m             \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_cap_for_this_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_for_this_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_for_this_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minertia_of_momentum\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0miner\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml2_penalty_factor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-cca7108ad7c8>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, x_batch, y_batch, learning_rate, momentum, l2_penalty)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid_object_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml2_penalty\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[1;31m#aba k garne backward ma gar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'grad_output' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Aashish Adhikari\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import time\n",
    "try:\n",
    "   import _pickle as pickle\n",
    "except:\n",
    "   import pickle\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "class LinearTransform(object):\n",
    "\n",
    "    def __init__(self, W, b): #linear transform ma kina weight diyo budo le bhaneko ta duit hau ma obj banauda farak farak\n",
    "                                #diyera banauna ko lagi rahexa\n",
    "        self.first_layer_weights = W #weights for all n nodes in the hidden layer\n",
    "        self.first_layer_bias = b\n",
    "        \n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        #print(\"aa\",x.shape,self.weights.shape)\n",
    "        #Never do this because it does element-wise and this gives an error\n",
    "        #batch_linear_summation = x * self.weights# + self.bias\n",
    "        \n",
    "        \n",
    "        batch_linear_summation = np.dot(x, self.first_layer_weights) + self.first_layer_bias\n",
    "        #print(\"batch_linear_summation is \",batch_linear_summation)\n",
    "        \n",
    "        return (batch_linear_summation)#,self.relu_object.forward(batch_linear_summation_without_relu,y))\n",
    "        \n",
    "    def forward_2(self,x):\n",
    "        batch_linear_summation_without_sigmoid = np.dot(x,self.weights)+self.bias\n",
    "        return (batch_linear_summation_without_sigmoid,self.sig.forward(batch_linear_summation_without_sigmoid))\n",
    "        print()\n",
    "    def backward(\n",
    "        self, \n",
    "        grad_output, \n",
    "        learning_rate=0.0, \n",
    "        momentum=0.0, \n",
    "        l2_penalty=0.0,\n",
    "    ):\n",
    "        print()\n",
    "\t# DEFINE backward function\n",
    "\n",
    "\n",
    "class ReLU(object):\n",
    "    def __init__(self):\n",
    "        print()\n",
    "        \n",
    "    def forward(self, x):\n",
    "\t# DEFINE forward function\n",
    "        relu_output = np.maximum(0,x)\n",
    "        #print(\"relu_output is\",relu_output)\n",
    "        return relu_output\n",
    "        \n",
    "           \n",
    "    def backward(\n",
    "        self, \n",
    "        grad_output, \n",
    "        learning_rate=0.0, \n",
    "        momentum=0.0, \n",
    "        l2_penalty=0.0,\n",
    "    ):\n",
    "        print()\n",
    "    # DEFINE backward function\n",
    "# ADD other operations in ReLU if needed\n",
    "\n",
    "\n",
    "\n",
    "# This is a class for a sigmoid layer followed by a cross entropy layer, the reason \n",
    "# this is put into a single layer is because it has a simple gradient form\n",
    "\n",
    "\n",
    "\n",
    "class SigmoidCrossEntropy(object):\n",
    "    def __init__(self):\n",
    "        print()\n",
    "        self.input_values=[]\n",
    "        #self.linear_transform_object_2 = LinearTransform(self.second_layer_weights,self.second_layer_bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         linear_summation = self.linear_transform_object_2.forward(self,x)\n",
    "#         print(\"linear summation size \",linear_summation.shape)\n",
    "#         #linear transform gar paile\n",
    "        #ani balla sigmoid\n",
    "        \n",
    "        \n",
    "        \n",
    "        #print(\"sigmoid ma aayeko input \", x)\n",
    "        self.input_values = x\n",
    "        sigmoid =  (1 / (1 + np.exp(-x)))\n",
    "        #print(\"sigmoid is \", sigmoid)\n",
    "        return sigmoid\n",
    "        \n",
    "    def backward(self,grad_output, learning_rate,direction, momentum, l2_penalty, second_layer_bias, grad_bias):\n",
    "        \n",
    "        #direction_updated = momentum * direction - np.dot(learning_rate, ( grad_output + np.dot(l2_penalty, second_layer_wts )))\n",
    "        #print(\"direction updated orr weight update is \",direction_updated)\n",
    "        #return(direction_updated, second_layer_wts +  direction_updated )\n",
    "        \n",
    "        self.linear_transform_object_for_sigmoid.backward(self,grad_output, learning_rate,direction, momentum, l2_penalty, second_layer_bias, grad_bias)\n",
    "        \n",
    "        \n",
    "        \n",
    "        print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# This is a class for the Multilayer perceptron\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MLP(object):\n",
    "\n",
    "    def __init__(self, input_dims, hidden_units):\n",
    "    # INSERT CODE for initializing the network\n",
    "        \n",
    "        self.input_dims = input_dims\n",
    "        self.hidden_units = hidden_units\n",
    "        \n",
    "        \n",
    "        self.linear_transform_layer = LinearTransform(np.random.rand(input_dims, hidden_units),np.random.rand(hidden_units))\n",
    "        \n",
    "        self.relu_layer = ReLU()\n",
    "        \n",
    "        self.linear_transform_object_for_sigmoid = LinearTransform(np.random.rand(hidden_units,1),np.random.rand(1))\n",
    "\n",
    "        \n",
    "        self.sigmoid_object_layer = SigmoidCrossEntropy()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #------------------------------------\n",
    "        self.x_for_this_batch = []\n",
    "        self.y_for_this_batch = []\n",
    "        self.y_cap_for_this_batch = []\n",
    "        \n",
    "        \n",
    "        self.hidden_layer_output = [] #Relu through aayeko xa sigmoid of A1\n",
    "        self.sigmoid_output = []#Sigmoid through aayeko is Relu of A2, equals y cap for this batch\n",
    "        \n",
    "        self.hidden_layer_op_before_Relu = [] #A1\n",
    "        self.sigmoid_layer_op_before_sigmoid = []#A2\n",
    "        #------------------\n",
    "\n",
    "        \n",
    "    def train(self,x_batch,y_batch,learning_rate,momentum,l2_penalty):\n",
    "        \n",
    "        self.x_for_this_batch = x_batch\n",
    "        self.y_for_this_batch = y_batch\n",
    "        #print(\"within train x_batch, y_batch\",x_batch,y_batch)\n",
    "        #print(\"x batch and y batch \",len(x_batch),len(y_batch))\n",
    "        \n",
    "        \n",
    "        ###########------------First FOrward Pass and then bakward pass---------\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.hidden_layer_op_before_Relu= self.linear_transform_layer.forward(x_batch)\n",
    "        \n",
    "        self.hidden_layer_output = self.relu_layer.forward(self.hidden_layer_op_before_Relu)\n",
    "        \n",
    "        \n",
    "        print(\"Z 1 yeha  \",self.hidden_layer_output)\n",
    "        \n",
    "        self.sigmoid_layer_linear_transform_op = self.linear_transform_object_for_sigmoid.forward(self.hidden_layer_output)\n",
    "        \n",
    "        self.sigmoid_layer_op = self.sigmoid_object_layer.forward(self.sigmoid_layer_linear_transform_op)\n",
    "                \n",
    "        \n",
    "        #avoid overflow and underflow\n",
    "        \n",
    "        self.sigmoid_layer_op = np.where(self.sigmoid_layer_op == 0,0.00000001,self.sigmoid_layer_op)\n",
    "        \n",
    "        \n",
    "        self.sigmoid_layer_op = np.where(self.sigmoid_layer_op == 1,1-0.00000001,self.sigmoid_layer_op)\n",
    "\n",
    "        #self.y_cap_for_this_batch = self.sigmoid_layer_op\n",
    "        \n",
    "        \n",
    "        print(\"y_cap or sigmoid_output\",self.sigmoid_layer_op)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #---------------weight update yehi garne ani loss pass garne return ma\n",
    "        \n",
    "        \n",
    "        #----------weight update, for that call all the backward passees one by one\n",
    "        \n",
    "        \n",
    "        p = np.transpose(self.y_for_this_batch)\n",
    "         \n",
    "        loss = -(np.dot(p,np.log(self.sigmoid_layer_op)) + np.dot((1-p),(np.log(1-self.sigmoid_layer_op))))\n",
    "        \n",
    "        print(\" y y cap Z1 loss \",self.y_for_this_batch,self.sigmoid_layer_op,self.hidden_layer_output, loss)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #aba palaipalo backward function call gardai weight update gar yeha\n",
    "        \n",
    "        \n",
    "        #paile sab calcu;late garne sab value firta aauxa ani balla sabai update garne\n",
    "        #natra bigrainxa palaipalo update gardai garda as andrew said\n",
    "        \n",
    "        \n",
    "         \n",
    "        \n",
    "        #uodated wt = (call backward func with args after calculating the intermediate vaues)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #calculate the grad output to send to the backward fucntion of sigmod cross entropy\n",
    "        grad_output_for_W2 = np.dot(self.sigmoid_output  ,( np.ones(np.array(self.sigmoid_output)) - (self.sigmoid_output)))\n",
    "        \n",
    "        #pass for W2\n",
    "        dE_by_dW2 = self.sigmoid_object_layer.backward(grad_output, learning_rate, momentum, l2_penalty)\n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        dE_by_db2 = grad_output_for_W2 #Z1 from Relu\n",
    "        \n",
    "        \n",
    "        #do similarly for W1 and b1\n",
    "        \n",
    "        grad_output_for_lin_tr = np.dot(grad_output_for_W2 ,\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        #call sigmoid backward that calls linear backward \n",
    "        \n",
    "        \n",
    "        time.sleep(1000)\n",
    "        \n",
    "        \n",
    "        #return self.sigmoid_layer_op\n",
    "        \n",
    "        \n",
    "        \n",
    "\t# INSERT CODE for training the network\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def evaluate(self, y_cap, y):\n",
    "        \n",
    "        print(\"y cap and y for loss \", y_cap, y)\n",
    "        p = np.transpose(y)\n",
    "         \n",
    "        loss = -(np.dot(p,np.log(y_cap)) + np.dot((1-p),(np.log(1-y_cap))))\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(\"loss is  \",loss) \n",
    "        \n",
    "        return loss\n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "    def weight_update(self, loss_for_the_batch, lr, direction_passed, momentum,l2_penalty):\n",
    "        \n",
    "        print(\"y truth\",self.y_for_this_batch,\"y cap\",self.y_cap_for_this_batch)\n",
    "        \n",
    "\n",
    "        \n",
    "        x1 = np.dot(self.y_for_this_batch, np.transpose(( 1- self.y_for_this_batch)))\n",
    "        x2 = (np.dot((self.y_cap_for_this_batch),np.transpose((1-self.y_cap_for_this_batch))))\n",
    "        a = x1/x2\n",
    "        \n",
    "        \n",
    "        b = np.dot(self.y_cap_for_this_batch, np.transpose((1 - self.y_cap_for_this_batch)))\n",
    "       \n",
    "        dE_by_dW2 = np.dot( np.dot(a,b),self.hidden_layer_output)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         print(\"size \",x1.size,x2.size,a.size,b.size,dE_by_db2.size,dE_by_dW2.size)\n",
    "#         print(\"a, b, grad \",a,b,dE_by_db2, self.second_layer_bias)\n",
    "        \n",
    "        #The dimension of dE by dW2 is 10 by 2\n",
    "        #10 examples taken in a batch so each of them give a unique feedback for both the weights in W2 for the 2 input weights\n",
    "        #to the sigmoid node\n",
    "        #we need to sum them up and take an average to do the update\n",
    "        \n",
    "        \n",
    "        \n",
    "        dE_by_dW2 = (dE_by_dW2.sum(axis = 0))/10#--------------------------------dont forget to change batch size here\n",
    "        \n",
    "        #call the sigmoid unit to do the update on the weights by passing the necessary arguments\n",
    "        \n",
    "        \n",
    "        #second layer weights updated here\n",
    "        direction_passed, self.second_layer_weights =  self.sigmoid_object_for_backpass.backward(self.second_layer_weights, lr, direction_passed, momentum,l2_penalty, dE_by_dW2)\n",
    "        \n",
    "        #------------------second layer bias update\n",
    "        \n",
    "        \n",
    "        dE_by_db2 = np.dot(a,b)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #time.sleep(5)\n",
    "        #NOW UPdate the first ;layer weights\n",
    "        \n",
    "        \n",
    "        #dE_by_d_y_cap = y - \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\t# INSERT CODE for testing the network\n",
    "# ADD other operations and data entries in MLP if needed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if sys.version_info[0] < 3:\n",
    "        print(\"system version is less than 3\") \n",
    "        data = pickle.load(open('dataset_folder/cifar_2class_py2.p', 'rb'))\n",
    "        \n",
    "    else:\n",
    "        #train_x, train_y, test_x, test_y \n",
    "        data = pickle.load(open('../../dataset_folder/cifar_2class_py2.p', 'rb'), encoding='bytes')\n",
    "        \n",
    "    #print(data)\n",
    "    #print(data[b'test_data'])\n",
    "    \n",
    "    \n",
    "    train_x = np.array(data[b'train_data'])\n",
    "    train_y = np.array(data[b'train_labels'])\n",
    "    test_x = np.array(data[b'test_data'])\n",
    "    test_y = np.array(data[b'test_labels'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    def normalize(x):\n",
    "         \n",
    "        top = x - np.amin(x)\n",
    "        bottom = np.amax(x)-np.amin(x)\n",
    "        return(top/bottom)\n",
    "    \n",
    "    \n",
    "    #for checking------------------remove paxi\n",
    "    #print(\"dtype is \",train_x.dtype)\n",
    "    train_x = train_x[...,0:1000]\n",
    "    train_y = train_y[...,0:1000]\n",
    "    test_x = test_x[...,0:1000]\n",
    "    test_y = test_y[...,0:1000]\n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(\" minimum in x is \", np.amax(train_x))\n",
    "#     print(\" maximum in x is \", np.amax(train_x))\n",
    "#     train_x = normalize(train_x)\n",
    "#     print(\" normalized x is \",train_x)\n",
    "#     print(\" maximum in x is \", np.amax(train_x))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    num_examples, input_dims = train_x.shape\n",
    "    \n",
    "    \n",
    "\t# INSERT YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    #ask the user about the the number of hidden nodes hs/ she wants\n",
    "    \n",
    "    \n",
    "    #inp = input(\"How many hidden nodes do you want?\\n\")\n",
    "    #num_of_hidden_nodes = int(inp)\n",
    "    \n",
    "    \n",
    "    num_of_hidden_nodes = 2\n",
    "#     print(\"dimension of each example is \",input_dims)\n",
    "    \n",
    "    \n",
    "\t\n",
    "    \n",
    "    \n",
    "    # YOU CAN CHANGE num_epochs AND num_batches TO YOUR DESIRED VALUES\n",
    "    \n",
    "    \n",
    "    num_epochs = 10\n",
    "    num_batches = 1000\n",
    "    learning_rate = [0.001,0.003,0.01,0.03,0.1,0.3,1,3,10]\n",
    "    inertia_of_momentum = [0.001,0.003,0.01,0.03,0.1,0.3,1,3,10]\n",
    "    l2_penalty_factor = [0.0000001,0.0000003,0.000001,0.000003,0.00001,0.00003,0.0001,0.0003,0.001,0.003,0.01,0.03,1,3,10]\n",
    "    print(\"Choose the corresponding index number for the learning rate you want to use\")\n",
    "    #lr = int(input(\"[0.001,0.003,0.01,0.03,0.1,0.3,1,3,10]\"))\n",
    "    lr =5 \n",
    "    print(\"Choose the corresponding index number for the inertia of momentum you want to use\")\n",
    "    #iner = int(input(\"[0.001,0.003,0.01,0.03,0.1,0.3,1,3,10]\"))\n",
    "    iner = 1\n",
    "    print(\"Choose the corresponding index number for the L2 penalty factor you want to use\")\n",
    "    #penalty = int(input(\"[0.0000001,0.0000003,0.000001,0.000003,0.00001,0.00003,0.0001,0.0003,0.001,0.003,0.01,0.03,1,3,10]\"))\n",
    "    penalty = 1\n",
    "    mlp = MLP(input_dims, num_of_hidden_nodes)\n",
    "    \n",
    "    \n",
    "    \n",
    "    epoch_num = 1\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\" Epoch is \",epoch_num)\n",
    "\n",
    "\t# INSERT YOUR CODE FOR EACH EPOCH HERE\n",
    "        total_loss_for_epoch = 0.0\n",
    "        direction = 0\n",
    "        for b in range(num_batches):\n",
    "            \n",
    "            \n",
    "            \n",
    "            #print(\"num of examples num of batches \",num_examples, \" \",num_batches)\n",
    "            \n",
    "            \n",
    "            \n",
    "            batch_start = int( (num_examples / num_batches) * b)\n",
    "            batch_end = int((num_examples / num_batches)*(b +1))\n",
    "            \n",
    "            \n",
    "            \n",
    "#             print(\"batch start and end\",batch_start,\" \",batch_end)\n",
    "#             print(\"Chosen learning rate, inertia of momentum and l2 penalty factor are\")\n",
    "            #print(learning_rate[lr], inertia_of_momentum[iner], l2_penalty_factor[penalty])\n",
    "            print(\"train size is  \",train_x.shape)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            mlp.x_for_this_batch = train_x[batch_start:batch_end,...]\n",
    "            mlp.y_for_this_batch = train_y[batch_start:batch_end,...]\n",
    "            mlp.y_cap_for_this_batch = mlp.train(mlp.x_for_this_batch,mlp.y_for_this_batch,int(learning_rate[lr]), inertia_of_momentum[iner], l2_penalty_factor[penalty])\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            #print(\"y_cap_for_batch\",mlp.y_cap_for_this_batch)\n",
    "            \n",
    "            loss = mlp.evaluate(mlp.y_cap_for_this_batch,mlp.y_for_this_batch )\n",
    "            \n",
    "            \n",
    "            \n",
    "            total_loss_for_epoch = total_loss_for_epoch + loss\n",
    "            \n",
    "            #call for weight updates,  need to pass ionly the loss\n",
    "            \n",
    "            mlp.weight_update(loss, learning_rate[lr], direction, inertia_of_momentum[iner], l2_penalty_factor[penalty])\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # INSERT YOUR CODE FOR EACH MINI_BATCH HERE\n",
    "            # MAKE SURE TO UPDATE total_loss\n",
    "            \n",
    "            \n",
    "            \n",
    "            #print('\\r[Epoch {}, mb {}]    Avg.Loss = {:.3f}'.format(epoch + 1,b + 1,total_loss_for_epoch,),end='', )\n",
    "            \n",
    "            #after each mini bach update you want to update the momentum value\n",
    "            sys.stdout.flush()\n",
    "            # INSERT YOUR CODE AFTER ALL MINI_BATCHES HERE\n",
    "        epoch_num +=1\n",
    "        \n",
    "        direction = 0\n",
    "        # MAKE SURE TO COMPUTE train_loss, train_accuracy, test_loss, test_accuracy\n",
    "    \n",
    "        \n",
    "        print()\n",
    "        #do for each epoch this \n",
    "        \n",
    "        \n",
    "        print('    Train Loss: {:.3f}    Train Acc.: {:.2f}%'.format(\n",
    "            train_loss,\n",
    "            100. * train_accuracy,\n",
    "        ))\n",
    "        print('    Test Loss:  {:.3f}    Test Acc.:  {:.2f}%'.format(\n",
    "            test_loss,\n",
    "            100. * test_accuracy,\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
