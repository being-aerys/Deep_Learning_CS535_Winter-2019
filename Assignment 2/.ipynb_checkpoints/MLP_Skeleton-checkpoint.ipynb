{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype is  uint8\n",
      " normalized x is  [[134 131 128 ... 208 210 213]\n",
      " [202 202 204 ... 198 170 131]\n",
      " [126 122 126 ... 129 131 132]\n",
      " ...\n",
      " [255 255 253 ...  85  91  78]\n",
      " [155 154 154 ... 131 130 129]\n",
      " [156 155 156 ... 156 150 150]]\n",
      "dimension of each example is  1000\n",
      "Choose the corresponding index number for the learning rate you want to use\n",
      "Choose the corresponding index number for the inertia of momentum you want to use\n",
      "Choose the corresponding index number for the L2 penalty factor you want to use\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "num of examples num of batches  10000   1000\n",
      "batch start and end 0   10\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[134 131 128 ... 208 210 213]\n",
      " [202 202 204 ... 198 170 131]\n",
      " [126 122 126 ... 129 131 132]\n",
      " ...\n",
      " [ 44  47  51 ...  41  42  38]\n",
      " [106 105 108 ... 146 146 142]\n",
      " [138 135 135 ...  94  95  96]] [[1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "within train hiden layer output [[53898.06460532 52359.17865089]\n",
      " [76366.29455643 72831.46064606]\n",
      " [66570.47330594 65011.52729136]\n",
      " [71619.36931808 69852.0535165 ]\n",
      " [96563.28019504 95326.41045153]\n",
      " [46293.00157681 46361.4308244 ]\n",
      " [62097.81324604 60153.06546853]\n",
      " [42476.43871786 43074.32240784]\n",
      " [69400.16417309 69508.97331169]\n",
      " [47084.92023589 48101.98853865]]\n",
      "sigmoid ma aayeko input  [[ 67501.5561312 ]\n",
      " [ 94562.66577073]\n",
      " [ 83644.33343716]\n",
      " [ 89916.41828915]\n",
      " [122144.83623765]\n",
      " [ 59083.37105318]\n",
      " [ 77634.16445358]\n",
      " [ 54638.17248841]\n",
      " [ 88579.72797166]\n",
      " [ 60848.09556546]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      " y is  [[1 0 0 0 0 1 1 0 1 0]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 1]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 10   20\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[213 211 211 ... 100 105  96]\n",
      " [107 106 106 ... 105 104 104]\n",
      " [135 135 134 ... 134 133 133]\n",
      " ...\n",
      " [236 231 233 ...  60  51  57]\n",
      " [227 223 224 ... 232 232 232]\n",
      " [113 113 113 ...  31  27  39]] [[1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "within train hiden layer output [[84587.35228274 83635.3071947 ]\n",
      " [52402.35357167 51883.63451638]\n",
      " [46023.33639591 45361.06883685]\n",
      " [85222.97419787 81845.93366433]\n",
      " [57441.15293076 56639.87020294]\n",
      " [70077.38325604 68621.4345709 ]\n",
      " [55027.6745929  52906.27654311]\n",
      " [63588.25401285 61829.23782322]\n",
      " [74127.99508688 72371.3060532 ]\n",
      " [63806.33051322 61388.16965149]]\n",
      "sigmoid ma aayeko input  [[107100.85315116]\n",
      " [ 66406.21919809]\n",
      " [ 58158.01957237]\n",
      " [105981.4989845 ]\n",
      " [ 72606.38521356]\n",
      " [ 88198.00329669]\n",
      " [ 68478.36466228]\n",
      " [ 79682.45112023]\n",
      " [ 93123.65422313]\n",
      " [ 79435.96707919]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      " y is  [[1 1 1 0 0 0 1 1 1 1]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 2]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 20   30\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[161 165 174 ...  82  84  84]\n",
      " [180 182 180 ...  47  45  45]\n",
      " [  2   1   1 ...  91  84  77]\n",
      " ...\n",
      " [181 179 180 ...  98  97  94]\n",
      " [167 163 163 ...  88  89  82]\n",
      " [ 84  89  95 ... 214 205 199]] [[0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "within train hiden layer output [[64450.41164403 62433.8135126 ]\n",
      " [51530.43234942 49570.08099201]\n",
      " [28664.59122842 29279.89858044]\n",
      " [81304.38409322 79842.58773205]\n",
      " [60793.76038286 60128.46300267]\n",
      " [69733.59436097 67481.79177294]\n",
      " [73364.0893392  72587.12505912]\n",
      " [61627.56126393 60240.07233899]\n",
      " [66524.25337099 64266.01791954]\n",
      " [67566.18103789 65754.85244324]]\n",
      "sigmoid ma aayeko input  [[ 80576.81162195]\n",
      " [ 64147.1519259 ]\n",
      " [ 37040.38494256]\n",
      " [102509.03398389]\n",
      " [ 76989.55439432]\n",
      " [ 87126.28289661]\n",
      " [ 92929.26665484]\n",
      " [ 77478.01441415]\n",
      " [ 83028.89408673]\n",
      " [ 84713.12300575]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      " y is  [[0 1 0 0 0 1 1 1 0 0]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 3]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 30   40\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[164 166 168 ...  71  84 112]\n",
      " [121 123 116 ...  46  45  45]\n",
      " [254 255 255 ... 104  79  58]\n",
      " ...\n",
      " [168 166 167 ... 150 148 142]\n",
      " [ 58  49  37 ...  62  50  32]\n",
      " [123 113 106 ...  62  51  43]] [[1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "within train hiden layer output [[68413.31565317 68664.83566331]\n",
      " [29563.36925297 29306.76587592]\n",
      " [86457.97542577 85199.29583984]\n",
      " [51936.8418772  53242.21897908]\n",
      " [71783.6236302  68630.53733875]\n",
      " [44577.16584242 43844.88209312]\n",
      " [85836.16260049 83118.04641708]\n",
      " [67902.72417019 66516.14693173]\n",
      " [41227.54120409 40928.65197842]\n",
      " [66316.49929562 65077.04613116]]\n",
      "sigmoid ma aayeko input  [[ 87434.95669978]\n",
      " [ 37492.54960195]\n",
      " [109242.08353445]\n",
      " [ 67264.27938552]\n",
      " [ 89023.04652036]\n",
      " [ 56258.27022836]\n",
      " [107287.76360658]\n",
      " [ 85480.27594697]\n",
      " [ 52332.02447766]\n",
      " [ 83574.7447319 ]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      " y is  [[1 0 1 1 0 0 1 1 1 1]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 4]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 40   50\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[252 240 231 ... 134 142 143]\n",
      " [151 151 151 ... 170 171 170]\n",
      " [228  45   2 ... 252 252 251]\n",
      " ...\n",
      " [ 93  95  95 ... 148 149 156]\n",
      " [ 77  96  93 ...   7   6  23]\n",
      " [ 91 100 117 ...  68  67  67]] [[1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "within train hiden layer output [[59999.31159143 57049.67786238]\n",
      " [82022.26376057 80777.94564441]\n",
      " [49596.77947964 48156.02933817]\n",
      " [86346.92170505 85043.75314076]\n",
      " [90703.63710436 88561.06146326]\n",
      " [85834.34753958 83109.15606295]\n",
      " [93558.54418652 92116.77117256]\n",
      " [44224.77696975 44038.72276758]\n",
      " [63593.55984021 62949.60258653]\n",
      " [53924.49564352 53906.7966234 ]]\n",
      "sigmoid ma aayeko input  [[ 74158.63816371]\n",
      " [103597.48307143]\n",
      " [ 62095.03329885]\n",
      " [109065.07267855]\n",
      " [113952.36187758]\n",
      " [107279.81858887]\n",
      " [118150.39634392]\n",
      " [ 56243.63159641]\n",
      " [ 80576.59524682]\n",
      " [ 68745.8392847 ]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      " y is  [[1 0 0 0 1 0 1 1 0 0]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, mb 5]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 50   60\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[ 61  65  69 ...  54  29  22]\n",
      " [186 184 187 ... 215 222 227]\n",
      " [ 58  51  50 ...  59  59  61]\n",
      " ...\n",
      " [120 116 115 ... 168 166 164]\n",
      " [174 172 173 ... 120 121 121]\n",
      " [196 203 212 ... 202 216 231]] [[1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "within train hiden layer output [[65036.12446157 64008.87275882]\n",
      " [95729.24362327 93205.01803225]\n",
      " [50811.5089207  51918.66467798]\n",
      " [61042.96328724 59824.31396605]\n",
      " [76559.204676   73798.85924527]\n",
      " [70067.3355347  68546.82741826]\n",
      " [62934.03318179 61913.97538575]\n",
      " [67299.27278394 65906.60671178]\n",
      " [71799.00865871 69859.05040511]\n",
      " [94094.95977306 92082.27009804]]\n",
      "sigmoid ma aayeko input  [[ 82110.99604005]\n",
      " [120056.85137733]\n",
      " [ 65671.60658379]\n",
      " [ 76866.95125709]\n",
      " [ 95424.99593665]\n",
      " [ 88133.8140793 ]\n",
      " [ 79436.31907013]\n",
      " [ 84705.96222295]\n",
      " [ 90008.08744229]\n",
      " [118380.04333029]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      " y is  [[1 0 1 0 0 0 0 0 0 1]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 6]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 60   70\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[ 61  62  62 ...  59  60  61]\n",
      " [255 252 253 ... 255 255 255]\n",
      " [ 72  72  72 ... 191 195 194]\n",
      " ...\n",
      " [243 240 240 ... 181 165 185]\n",
      " [176 173 174 ... 188 187 188]\n",
      " [ 49  52  49 ...  99  93  94]] [[1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "within train hiden layer output [[ 42679.79029873  42414.59607011]\n",
      " [107189.71631769 103926.01654237]\n",
      " [ 65685.35987281  64916.55698396]\n",
      " [ 79291.62123162  77377.02281395]\n",
      " [ 59488.22979339  57991.89662355]\n",
      " [ 62483.01609271  60261.64040987]\n",
      " [ 36420.85872345  34690.2124174 ]\n",
      " [ 82365.38583545  78575.46321899]\n",
      " [ 69876.48187962  67392.83324676]\n",
      " [ 38784.24686105  39061.91458128]]\n",
      "sigmoid ma aayeko input  [[ 54210.61975849]\n",
      " [134081.75520176]\n",
      " [ 83144.52092657]\n",
      " [ 99582.18990665]\n",
      " [ 74663.50733639]\n",
      " [ 77905.19712099]\n",
      " [ 45063.55444033]\n",
      " [102009.17655033]\n",
      " [ 87123.97435804]\n",
      " [ 49675.38197442]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      " y is  [[1 0 0 0 1 0 1 0 0 0]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 7]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 70   80\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[171 171 171 ... 108 109 110]\n",
      " [ 51  66  65 ... 164 161 160]\n",
      " [250 254 252 ... 253 253 253]\n",
      " ...\n",
      " [162 160 162 ... 165 165 166]\n",
      " [221 225 229 ... 173 176 171]\n",
      " [209 208 208 ... 142 140 139]] [[0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n",
      "within train hiden layer output [[ 60643.78594008  57958.50225199]\n",
      " [ 56610.6606246   55769.21635229]\n",
      " [105659.48782067 103013.10294821]\n",
      " [ 49273.33508086  48089.55382694]\n",
      " [ 75852.72801819  74801.66526634]\n",
      " [ 89612.10551891  88242.78604678]\n",
      " [ 94388.94466352  90900.86414253]\n",
      " [ 68271.71946244  66995.63654757]\n",
      " [ 92871.96580971  90112.85966351]\n",
      " [ 70643.401879    69908.95371558]]\n",
      "sigmoid ma aayeko input  [[ 75190.78843611]\n",
      " [ 71515.45335836]\n",
      " [132621.80970447]\n",
      " [ 61887.10466861]\n",
      " [ 95884.42870972]\n",
      " [113175.90422542]\n",
      " [117580.77152765]\n",
      " [ 86038.71913958]\n",
      " [116226.58089029]\n",
      " [ 89493.92010546]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n",
      " y is  [[0 1 0 0 1 1 0 0 1 1]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 8]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 80   90\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[249 250 251 ...  75  84  90]\n",
      " [140 140 141 ... 164 160 154]\n",
      " [233 231 231 ... 233 234 235]\n",
      " ...\n",
      " [ 93  97 102 ...  30  28  32]\n",
      " [154 150 150 ...  62  57  53]\n",
      " [ 75  72  65 ...  81  81  81]] [[0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "within train hiden layer output [[73894.20878643 72149.79602891]\n",
      " [65265.79334596 62850.48008526]\n",
      " [89707.87984382 88231.93287397]\n",
      " [55221.26701405 54375.67522491]\n",
      " [62358.41479791 62141.32707296]\n",
      " [52405.44553274 50900.57495847]\n",
      " [64486.38913335 62378.75183473]\n",
      " [56929.4046989  56154.17138528]\n",
      " [56892.14606681 54735.51314958]\n",
      " [46074.23507787 46471.48606869]]\n",
      "sigmoid ma aayeko input  [[ 92835.32011064]\n",
      " [ 81299.21192014]\n",
      " [113213.17165523]\n",
      " [ 69740.52013864]\n",
      " [ 79341.35528653]\n",
      " [ 65625.36941486]\n",
      " [ 80550.23678355]\n",
      " [ 71974.5793305 ]\n",
      " [ 70827.734382  ]\n",
      " [ 59066.09992697]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      " y is  [[0 0 0 1 0 1 1 1 0 0]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 9]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 90   100\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[141 157 153 ...  81  87  80]\n",
      " [178 177 178 ... 164 164 164]\n",
      " [108 107 110 ...   1   2   2]\n",
      " ...\n",
      " [194 196 200 ... 106  87  68]\n",
      " [220 221 224 ... 255 255 255]\n",
      " [ 81  55  59 ...   2   3   4]] [[1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n",
      "within train hiden layer output [[52034.19120418 51651.52142807]\n",
      " [77678.72908407 74847.8762837 ]\n",
      " [35921.31093055 34977.69355964]\n",
      " [58472.17647938 57952.36142324]\n",
      " [60082.16274608 58248.99328953]\n",
      " [86804.02016858 85324.00282829]\n",
      " [68720.84636096 67380.92430279]\n",
      " [74681.8593026  71459.46484406]\n",
      " [99213.28545186 96856.79904176]\n",
      " [52578.09259079 52855.43896259]]\n",
      "sigmoid ma aayeko input  [[ 66045.0407274 ]\n",
      " [ 96796.40433445]\n",
      " [ 45052.90301976]\n",
      " [ 74145.05147503]\n",
      " [ 75152.77965892]\n",
      " [109507.18586184]\n",
      " [ 86560.60223965]\n",
      " [ 92663.46547225]\n",
      " [124632.88423793]\n",
      " [ 67263.82550572]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n",
      " y is  [[1 0 1 1 1 0 1 1 0 1]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, mb 10]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 100   110\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[ 41  41  41 ...  24  22  28]\n",
      " [204 207 208 ...  78  75  71]\n",
      " [ 59  58  59 ...  56  95 117]\n",
      " ...\n",
      " [165 165 165 ... 142 140 139]\n",
      " [193 193 194 ... 124 143 139]\n",
      " [ 74  74  74 ... 115 113 111]] [[1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "within train hiden layer output [[31897.54792748 34251.89960493]\n",
      " [58988.36763801 58489.25869492]\n",
      " [65356.31446271 64664.1376669 ]\n",
      " [67527.87862736 67472.82080481]\n",
      " [56066.9027718  55831.77919387]\n",
      " [57551.08129955 55627.74485936]\n",
      " [66023.44816863 64967.48322059]\n",
      " [74422.57002661 73289.04266045]\n",
      " [67494.79837866 66904.84551566]\n",
      " [49148.79876628 48677.24390937]]\n",
      "sigmoid ma aayeko input  [[42546.72147232]\n",
      " [74819.73125267]\n",
      " [82785.93120566]\n",
      " [86061.94657448]\n",
      " [71304.61915608]\n",
      " [71853.61106676]\n",
      " [83347.09404271]\n",
      " [93995.19055679]\n",
      " [85594.08902469]\n",
      " [62295.10606665]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      " y is  [[1 1 1 0 1 1 0 0 1 0]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 11]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 110   120\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[190 190 191 ...  79  81  83]\n",
      " [211 202 204 ... 143 141 137]\n",
      " [121 120 121 ...  65  68  67]\n",
      " ...\n",
      " [197 194 194 ... 145 143 138]\n",
      " [208 210 211 ... 232 233 230]\n",
      " [189 188 187 ... 117 108 107]] [[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "within train hiden layer output [[68218.14167421 65830.7101446 ]\n",
      " [76641.24006357 74347.97619001]\n",
      " [55055.02209478 54262.01553337]\n",
      " [41672.61037674 40690.85107204]\n",
      " [62362.77970713 60166.13862637]\n",
      " [43818.42696049 43122.20082714]\n",
      " [58917.61070027 57502.52112686]\n",
      " [76001.99683362 73349.82930942]\n",
      " [83941.7106908  81075.01439612]\n",
      " [66192.30840093 64201.82730888]]\n",
      "sigmoid ma aayeko input  [[ 85085.97557528]\n",
      " [ 95901.30991115]\n",
      " [ 69570.38725201]\n",
      " [ 52356.10054258]\n",
      " [ 77771.56637619]\n",
      " [ 55319.48824362]\n",
      " [ 74000.5587651 ]\n",
      " [ 94800.58289221]\n",
      " [104753.8830877 ]\n",
      " [ 82818.7094961 ]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      " y is  [[1 1 1 1 1 1 1 1 0 0]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 12]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 120   130\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[221 219 221 ... 197 208 219]\n",
      " [ 92 113 121 ... 159 126 144]\n",
      " [ 46  58  66 ...  44  41  42]\n",
      " ...\n",
      " [ 84  86  91 ... 163 164 171]\n",
      " [124 148 159 ...  61  69  69]\n",
      " [176 172 176 ... 241 236 198]] [[1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "within train hiden layer output [[66508.81884162 64530.21431618]\n",
      " [81586.12596282 80571.66297191]\n",
      " [68464.72384073 67402.85870331]\n",
      " [98398.22131605 95430.01851829]\n",
      " [46032.22170925 44058.24580478]\n",
      " [54196.70268789 53675.64217124]\n",
      " [68490.37695638 68170.85828577]\n",
      " [66531.10195605 65548.07136669]\n",
      " [36215.03347922 35339.85272276]\n",
      " [63608.56827714 62239.70211485]]\n",
      "sigmoid ma aayeko input  [[ 83231.74730949]\n",
      " [103224.28023778]\n",
      " [ 86455.29868457]\n",
      " [123106.77414952]\n",
      " [ 57125.474548  ]\n",
      " [ 68692.35205894]\n",
      " [ 87078.77838744]\n",
      " [ 84052.45164168]\n",
      " [ 45481.89494861]\n",
      " [ 80018.84061939]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      " y is  [[1 1 0 0 0 1 1 0 0 1]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 13]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 130   140\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[227 222 217 ... 186 177 169]\n",
      " [127 126 128 ...  52  56  64]\n",
      " [  1   1   1 ... 239 239 239]\n",
      " ...\n",
      " [149 150 150 ... 163 163 164]\n",
      " [250 240 238 ... 234 235 234]\n",
      " [185 184 185 ... 183 182 182]] [[0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "within train hiden layer output [[99014.34890195 97435.92323626]\n",
      " [46604.65724853 46023.45951197]\n",
      " [59668.26520302 59047.2841581 ]\n",
      " [98207.36070087 94969.09847855]\n",
      " [42198.14437235 41757.2757367 ]\n",
      " [61275.20438125 60230.16994634]\n",
      " [61004.1530872  58870.48020298]\n",
      " [73749.11749752 71950.09607428]\n",
      " [73742.61036614 71722.69301026]\n",
      " [81420.6406367  79166.92358704]]\n",
      "sigmoid ma aayeko input  [[124998.40883906]\n",
      " [ 58963.78524514]\n",
      " [ 75589.68942289]\n",
      " [122648.48862377]\n",
      " [ 53456.66321222]\n",
      " [ 77301.24969604]\n",
      " [ 76089.2760873 ]\n",
      " [ 92606.85415989]\n",
      " [ 92422.76494263]\n",
      " [102027.05318189]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      " y is  [[0 1 0 0 1 0 0 0 0 0]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 14]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 140   150\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[171 167 166 ... 183 183 183]\n",
      " [163 168 174 ... 151 150 149]\n",
      " [191 191 191 ...  78  78  69]\n",
      " ...\n",
      " [114 113 111 ... 121 124 113]\n",
      " [ 99 100 104 ... 197 204 206]\n",
      " [235 235 236 ... 246 245 242]] [[0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "within train hiden layer output [[84198.71093657 83070.71248866]\n",
      " [71788.34720158 68790.16969919]\n",
      " [67119.27630446 65432.20892513]\n",
      " [87856.60725842 85774.56529422]\n",
      " [65724.4768329  64839.54695367]\n",
      " [73225.30276975 70947.85592548]\n",
      " [80399.83739365 78466.9241767 ]\n",
      " [58297.0007525  57006.14007952]\n",
      " [55431.92452041 53634.93136758]\n",
      " [84813.94322319 82803.08040235]]\n",
      "sigmoid ma aayeko input  [[106465.2658739 ]\n",
      " [ 89152.34805991]\n",
      " [ 84242.15809672]\n",
      " [110370.25323841]\n",
      " [ 83101.9840448 ]\n",
      " [ 91558.19441735]\n",
      " [100980.71455896]\n",
      " [ 73308.07452298]\n",
      " [ 69251.9943205 ]\n",
      " [106547.16088454]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      " y is  [[0 0 1 1 0 0 1 1 1 0]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, mb 15]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 150   160\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[ 57  58  55 ...  50  49  49]\n",
      " [255 252 252 ...   0   0   2]\n",
      " [201 202 207 ...  49  48  50]\n",
      " ...\n",
      " [100 100 101 ... 152 154 157]\n",
      " [181 182 189 ... 112 111 110]\n",
      " [131 131 132 ...  90  86  89]] [[0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n",
      "within train hiden layer output [[ 31845.39521599  31245.76191977]\n",
      " [107548.43378508 105768.69430166]\n",
      " [ 79520.31123046  78245.364135  ]\n",
      " [ 99157.63330425  96234.76765548]\n",
      " [ 76432.70034084  74528.13549277]\n",
      " [ 48538.08128772  47920.63704353]\n",
      " [ 55889.57338519  55058.05459607]\n",
      " [ 63482.21869248  62903.64397761]\n",
      " [ 68116.27496893  66411.2959808 ]\n",
      " [ 61612.89401038  60707.95951618]]\n",
      "sigmoid ma aayeko input  [[ 40129.40073301]\n",
      " [135720.11534098]\n",
      " [100382.83824115]\n",
      " [124111.18944724]\n",
      " [ 95944.73020144]\n",
      " [ 61400.27213692]\n",
      " [ 70603.88514191]\n",
      " [ 80486.65509117]\n",
      " [ 85499.18883913]\n",
      " [ 77843.33519996]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n",
      " y is  [[0 0 1 1 0 0 0 0 1 1]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 16]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 160   170\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[150 147 145 ...  57  60  59]\n",
      " [239 235 236 ... 229 232 240]\n",
      " [ 82  83  81 ...  65  66  68]\n",
      " ...\n",
      " [244 243 244 ...  69  68  70]\n",
      " [130 125 128 ... 124 127 123]\n",
      " [189 189 190 ... 175 174 176]] [[1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "within train hiden layer output [[ 54437.03965605  53431.9483079 ]\n",
      " [110301.43809482 106599.20749477]\n",
      " [ 33795.59360692  33550.72718853]\n",
      " [ 63959.28117043  62043.20473781]\n",
      " [ 59248.31959291  58444.99936099]\n",
      " [ 31718.21693489  30627.45875388]\n",
      " [ 48901.46490161  47371.45966338]\n",
      " [ 70699.04634979  68373.75359795]\n",
      " [ 46813.12768324  45570.27233858]\n",
      " [ 85323.51792947  82966.91309877]]\n",
      "sigmoid ma aayeko input  [[ 68613.61051223]\n",
      " [137700.5606051 ]\n",
      " [ 42898.4504105 ]\n",
      " [ 80030.56171734]\n",
      " [ 74909.10377521]\n",
      " [ 39576.39051272]\n",
      " [ 61137.39890104]\n",
      " [ 88298.85794512]\n",
      " [ 58703.05256618]\n",
      " [106921.77969647]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      " y is  [[1 0 0 1 1 1 0 1 0 0]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 17]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 170   180\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[245 144  28 ... 164 163 106]\n",
      " [219 222 206 ... 127 121 118]\n",
      " [142 135 132 ... 111 111 115]\n",
      " ...\n",
      " [171 170 171 ... 194 196 195]\n",
      " [ 49  45  44 ...  66  65  64]\n",
      " [163 166 171 ... 125 130 136]] [[1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n",
      "within train hiden layer output [[48448.50965872 48117.75737055]\n",
      " [74881.67896218 73710.54455823]\n",
      " [66519.01766621 65344.13755496]\n",
      " [87508.16882196 85257.21495844]\n",
      " [80262.96797556 77271.56607284]\n",
      " [41889.43555847 43559.1374289 ]\n",
      " [93835.11431457 91238.14076872]\n",
      " [85709.71868136 84378.26401497]\n",
      " [38674.53357936 38069.65290312]\n",
      " [74000.9679417  71924.30457464]]\n",
      "sigmoid ma aayeko input  [[ 61514.21150943]\n",
      " [ 94550.67772825]\n",
      " [ 83884.36643892]\n",
      " [109791.53286393]\n",
      " [ 99963.83145997]\n",
      " [ 54742.64470996]\n",
      " [117583.73044073]\n",
      " [108230.05695521]\n",
      " [ 48833.14159469]\n",
      " [ 92707.04058332]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n",
      " y is  [[1 0 1 1 0 1 0 0 1 1]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 18]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 180   190\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[111 110 110 ...  67  63  67]\n",
      " [ 75  77  79 ...  67  82  77]\n",
      " [252 248 250 ... 143 142 137]\n",
      " ...\n",
      " [178 175 176 ...  52  50  50]\n",
      " [198 196 196 ... 123 118 115]\n",
      " [ 97 115  78 ... 162 150 138]] [[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "within train hiden layer output [[60299.95108153 60008.39059818]\n",
      " [69013.27185838 67446.6239168 ]\n",
      " [97302.97442299 95074.10773814]\n",
      " [70466.67694411 68198.66121197]\n",
      " [65737.80005292 63752.02847565]\n",
      " [64886.80504318 64017.64495951]\n",
      " [42759.65539882 42249.18547446]\n",
      " [58458.29335386 57314.16343808]\n",
      " [80011.3408201  78034.49238134]\n",
      " [48389.02264603 48130.75415274]]\n",
      "sigmoid ma aayeko input  [[ 76657.31694077]\n",
      " [ 86753.0460756 ]\n",
      " [122298.58503595]\n",
      " [ 88048.14271725]\n",
      " [ 82242.90830686]\n",
      " [ 82046.40844121]\n",
      " [ 54117.26307971]\n",
      " [ 73630.51093782]\n",
      " [100450.37365778]\n",
      " [ 61496.04246077]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      " y is  [[1 1 1 1 0 0 0 1 0 0]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 19]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 190   200\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[ 21  18  15 ... 145 145 145]\n",
      " [161 161 162 ...  80  83  83]\n",
      " [ 77  78  78 ...   5   3   3]\n",
      " ...\n",
      " [112 114 117 ... 122 131 145]\n",
      " [ 74  79  82 ...  54  63  66]\n",
      " [176 181 186 ... 118 115 114]] [[0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "within train hiden layer output [[50578.05050927 49762.7267445 ]\n",
      " [66365.19686329 65936.6128188 ]\n",
      " [56195.27303458 55803.85034531]\n",
      " [47826.14257544 49475.65191271]\n",
      " [73175.65901691 70903.98689729]\n",
      " [69213.47796664 68006.52176795]\n",
      " [75305.0264114  74491.89450917]\n",
      " [72173.60457657 70146.3605843 ]\n",
      " [45155.01767881 43387.60074327]\n",
      " [70364.06897799 68339.28756467]]\n",
      "sigmoid ma aayeko input  [[63843.9863947 ]\n",
      " [84282.1399246 ]\n",
      " [71343.92073492]\n",
      " [62296.54203493]\n",
      " [91499.48867062]\n",
      " [87294.57873306]\n",
      " [95375.39628385]\n",
      " [90416.27607918]\n",
      " [56171.32344638]\n",
      " [88110.87513826]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      " y is  [[0 0 1 1 0 1 1 0 0 1]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, mb 20]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 200   210\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[222 221 225 ... 225 224 231]\n",
      " [ 43  37  42 ...  44  45  52]\n",
      " [128 127 131 ...  97 100  95]\n",
      " ...\n",
      " [ 93  71  76 ...  42  39  45]\n",
      " [126 128 129 ...  51  66  48]\n",
      " [146 159 194 ...  63  67  66]] [[0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n",
      "within train hiden layer output [[93861.33707    91543.6760127 ]\n",
      " [40548.99582315 42014.07830326]\n",
      " [47325.93499406 47293.27751458]\n",
      " [57096.39292031 56230.69048377]\n",
      " [62358.17246369 62489.48452329]\n",
      " [44102.48059334 44401.06767025]\n",
      " [53906.27660385 52802.37758061]\n",
      " [55904.58242897 54600.33233202]\n",
      " [59159.16966539 59329.16979899]\n",
      " [64795.36040739 62116.89833366]]\n",
      "sigmoid ma aayeko input  [[117839.44795366]\n",
      " [ 52870.59500779]\n",
      " [ 60320.03077711]\n",
      " [ 72115.51150741]\n",
      " [ 79618.30742506]\n",
      " [ 56473.37392935]\n",
      " [ 67858.19568436]\n",
      " [ 70246.81754767]\n",
      " [ 75570.00880509]\n",
      " [ 80489.93990448]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n",
      " y is  [[0 1 0 1 1 1 0 0 1 1]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 21]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 210   220\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[182 181 182 ...  30  28  28]\n",
      " [127 128 129 ...   2   3   4]\n",
      " [104 111 111 ... 164 133 110]\n",
      " ...\n",
      " [140 144 146 ... 138 135 135]\n",
      " [175 174 175 ...  67  67  67]\n",
      " [168 165 165 ...  69  78  78]] [[1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "within train hiden layer output [[54807.53010076 52678.965896  ]\n",
      " [59757.0683561  57930.68655318]\n",
      " [75433.4072778  74157.72069794]\n",
      " [34637.2894334  35326.39070257]\n",
      " [55011.17887333 53870.60950742]\n",
      " [73385.86771654 71525.65634756]\n",
      " [78026.44598183 76378.72055364]\n",
      " [68949.03111457 68463.45148209]\n",
      " [66468.74424085 65048.64808127]\n",
      " [40983.64642925 40114.96794028]]\n",
      "sigmoid ma aayeko input  [[68191.95284128]\n",
      " [74743.64958066]\n",
      " [95170.98916618]\n",
      " [44714.97072227]\n",
      " [69237.88716884]\n",
      " [92094.97421201]\n",
      " [98181.33071083]\n",
      " [87531.46034047]\n",
      " [83625.11602298]\n",
      " [51567.58549721]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      " y is  [[1 1 0 1 1 0 1 1 0 0]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 22]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 220   230\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[202 206 209 ...  40  52  40]\n",
      " [ 68  53 110 ... 116 113  97]\n",
      " [127 127 128 ...  24  33  36]\n",
      " ...\n",
      " [ 98  97  97 ...  41  43  43]\n",
      " [ 72  71  71 ...  50  54  57]\n",
      " [228 227 228 ... 233 231 228]] [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "within train hiden layer output [[ 77117.77078843  75593.38433899]\n",
      " [ 52092.24712496  50936.13483102]\n",
      " [ 52655.6142964   52232.29067854]\n",
      " [ 89035.7021073   86139.44316342]\n",
      " [ 91038.4237584   87697.47559857]\n",
      " [100290.09239352  97173.25636359]\n",
      " [ 44934.23649412  44638.50810005]\n",
      " [ 43980.81986383  43275.63623111]\n",
      " [ 54282.4587445   53285.12652807]\n",
      " [ 97521.76988931  94462.12052659]]\n",
      "sigmoid ma aayeko input  [[ 97120.82278393]\n",
      " [ 65503.55278039]\n",
      " [ 66805.07195337]\n",
      " [111225.76671198]\n",
      " [113425.57173339]\n",
      " [125400.83752094]\n",
      " [ 57060.99183552]\n",
      " [ 55519.42889964]\n",
      " [ 68422.67734451]\n",
      " [121916.42596924]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      " y is  [[0 1 1 0 0 0 1 1 0 0]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 23]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 230   240\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[223 224 225 ... 163 141 139]\n",
      " [190 191 194 ... 116 119 119]\n",
      " [146 146 147 ... 157 157 157]\n",
      " ...\n",
      " [218 216 215 ... 189 187 126]\n",
      " [228 230 238 ... 123 122 122]\n",
      " [ 38  39  41 ...  53  58  61]] [[1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "within train hiden layer output [[73302.32436665 72825.56397845]\n",
      " [76241.65059529 74948.20256055]\n",
      " [70716.47974145 69108.4654596 ]\n",
      " [11431.08886826 12084.20722124]\n",
      " [92054.84735024 89215.59103717]\n",
      " [68216.58842157 66281.41619194]\n",
      " [59313.77482304 58726.0563578 ]\n",
      " [87324.3575456  86087.03970868]\n",
      " [71748.1072642  70085.8779129 ]\n",
      " [45526.42153367 44719.54584103]]\n",
      "sigmoid ma aayeko input  [[ 93089.41560749]\n",
      " [ 96187.45518071]\n",
      " [ 88891.9071363 ]\n",
      " [ 15095.82846424]\n",
      " [115120.8787156 ]\n",
      " [ 85443.90891755]\n",
      " [ 75164.14525799]\n",
      " [110363.81779938]\n",
      " [ 90164.20278147]\n",
      " [ 57409.31637157]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      " y is  [[1 1 0 1 1 0 0 0 0 0]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 24]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 240   250\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[255 255 255 ... 254 254 254]\n",
      " [230 225 225 ... 156 154 158]\n",
      " [197 205 235 ...  17  11  12]\n",
      " ...\n",
      " [171 172 170 ...  91  76  76]\n",
      " [ 45  38  33 ...  25  31  37]\n",
      " [ 79  79  79 ...  84  83  84]] [[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "within train hiden layer output [[109452.45987842 107816.99968061]\n",
      " [ 87896.34268066  85528.82836962]\n",
      " [ 74756.62040648  73388.55378981]\n",
      " [ 69953.71170302  68842.6889778 ]\n",
      " [ 61045.01830345  59518.02443323]\n",
      " [ 55928.18961501  55745.93565547]\n",
      " [ 62902.23260842  60980.21853042]\n",
      " [ 62170.0648389   60346.56543475]\n",
      " [ 35808.45023323  35466.24550648]\n",
      " [ 43507.73555379  42731.47742747]]\n",
      "sigmoid ma aayeko input  [[138262.78136329]\n",
      " [110193.73769789]\n",
      " [ 94234.49280245]\n",
      " [ 88314.80483236]\n",
      " [ 76624.18697577]\n",
      " [ 71169.81867411]\n",
      " [ 78677.98107452]\n",
      " [ 77822.7843862 ]\n",
      " [ 45387.60509289]\n",
      " [ 54859.63136389]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      " y is  [[0 0 0 0 1 1 0 1 1 0]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, mb 25]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 250   260\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[156 146 110 ... 109 122 139]\n",
      " [144 141 140 ... 155 156 157]\n",
      " [ 83  77  77 ...  24  21  53]\n",
      " ...\n",
      " [214 201 205 ... 218 232 153]\n",
      " [206 196 191 ... 133 136 136]\n",
      " [ 56  63  58 ...  74  76  73]] [[1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "within train hiden layer output [[70413.81628446 67353.04774251]\n",
      " [72925.42313726 71680.39964349]\n",
      " [59808.74437033 58426.13953319]\n",
      " [85376.0563764  82348.34266887]\n",
      " [92058.14015057 89004.60558534]\n",
      " [58234.54983246 56911.94632801]\n",
      " [13633.71122377 13792.35175827]\n",
      " [61290.94528422 58525.17894454]\n",
      " [81301.35324205 80070.318786  ]\n",
      " [37045.823295   37594.84644012]]\n",
      "sigmoid ma aayeko input  [[ 87349.85631417]\n",
      " [ 91997.43039031]\n",
      " [ 75162.70585053]\n",
      " [106454.69481259]\n",
      " [114954.55194697]\n",
      " [ 73203.18123691]\n",
      " [ 17510.90686153]\n",
      " [ 75951.93997572]\n",
      " [102688.8126754 ]\n",
      " [ 47674.64531678]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      " y is  [[1 0 1 1 0 1 0 0 0 1]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 26]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 260   270\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[124 128 132 ...  83  99 103]\n",
      " [147 144 144 ...  54  57  57]\n",
      " [ 65  64  64 ... 222 219 218]\n",
      " ...\n",
      " [157 156 156 ...  68  71  70]\n",
      " [221 177 144 ... 108 108 112]\n",
      " [195 195 196 ... 127 137 143]] [[1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "within train hiden layer output [[62387.47383551 59958.28820594]\n",
      " [70301.25680382 69211.08743823]\n",
      " [63185.57914656 62830.50909845]\n",
      " [47179.49814369 46330.99902838]\n",
      " [61266.05054727 60319.02770228]\n",
      " [63268.29776192 62995.343862  ]\n",
      " [43237.39776253 43031.63423307]\n",
      " [49933.37366289 48411.96918413]\n",
      " [53034.99131341 50957.62034788]\n",
      " [76147.42793901 73766.51955803]]\n",
      "sigmoid ma aayeko input  [[77617.99213971]\n",
      " [88774.55916587]\n",
      " [80286.27438442]\n",
      " [59484.04985106]\n",
      " [77367.57644722]\n",
      " [80457.09900312]\n",
      " [54968.92750317]\n",
      " [62460.04282167]\n",
      " [65972.50748303]\n",
      " [95201.89545334]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      " y is  [[1 1 0 1 0 1 0 1 1 0]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 27]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 270   280\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[211 215 216 ... 137 132 127]\n",
      " [240 240 240 ...  90  72  68]\n",
      " [219 219 222 ...  76  75  75]\n",
      " ...\n",
      " [ 16  16  16 ...  41  42  41]\n",
      " [122 121 118 ... 253 253 253]\n",
      " [254 254 254 ... 255 255 255]] [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "within train hiden layer output [[ 56397.18104451  55268.40435578]\n",
      " [ 72714.21819125  71384.56132778]\n",
      " [ 68836.44600994  67482.24792608]\n",
      " [ 64818.01929708  62779.22345881]\n",
      " [ 73779.64982905  72961.18958056]\n",
      " [ 76700.84890664  75971.59000036]\n",
      " [ 61936.4933762   60855.90466823]\n",
      " [ 45824.46839796  45866.47348537]\n",
      " [ 63316.10980429  62259.42020317]\n",
      " [106135.35512305 103466.61772832]]\n",
      "sigmoid ma aayeko input  [[ 71014.58004265]\n",
      " [ 91660.76806173]\n",
      " [ 86696.64369859]\n",
      " [ 81027.88735183]\n",
      " [ 93426.12981502]\n",
      " [ 97221.97363944]\n",
      " [ 78116.17271998]\n",
      " [ 58464.91004377]\n",
      " [ 79894.35751201]\n",
      " [133210.80541639]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      " y is  [[0 1 1 0 0 1 1 1 0 0]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 28]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 280   290\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[ 59  61  63 ...  25  51  64]\n",
      " [120 124 126 ...  30  34  34]\n",
      " [131 129 129 ...  82  83  85]\n",
      " ...\n",
      " [ 10  11  12 ...  12  12  12]\n",
      " [153 147 146 ...  89  85  82]\n",
      " [179 180 181 ... 106 105  98]] [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n",
      "within train hiden layer output [[40568.94314504 40626.88889054]\n",
      " [49417.84968304 48299.94096845]\n",
      " [64188.39894505 62898.3588441 ]\n",
      " [87149.39919793 84543.7609448 ]\n",
      " [74722.86680068 73430.50283992]\n",
      " [63571.20421256 63599.35666149]\n",
      " [61937.7898886  61825.49188815]\n",
      " [22411.82750609 22923.21633581]\n",
      " [61072.95445935 60561.67593745]\n",
      " [68950.90866457 67951.17961204]]\n",
      "sigmoid ma aayeko input  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:212: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 51776.21211386]\n",
      " [ 62123.79921826]\n",
      " [ 80820.92052494]\n",
      " [109051.79887044]\n",
      " [ 94251.69834372]\n",
      " [ 81082.9630784 ]\n",
      " [ 78888.40442949]\n",
      " [ 28984.71068168]\n",
      " [ 77468.12837081]\n",
      " [ 87124.68754018]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n",
      " y is  [[0 1 1 1 1 1 1 0 1 1]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 29]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 290   300\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[191 160 152 ...   4   3   3]\n",
      " [170 170 171 ... 205 209 215]\n",
      " [ 18  19  19 ...  15  17  16]\n",
      " ...\n",
      " [198 190 183 ...  79  76  79]\n",
      " [206 201 199 ... 209 215 215]\n",
      " [138 136 136 ... 105 109 112]] [[0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "within train hiden layer output [[19014.41426641 17864.32369504]\n",
      " [86120.58554381 84307.41144722]\n",
      " [38799.38344504 38694.3017221 ]\n",
      " [54998.93635414 53126.64932726]\n",
      " [95427.17380338 94142.55361007]\n",
      " [91444.56008595 88932.72354952]\n",
      " [79101.95609728 76937.49446036]\n",
      " [40784.55659492 39236.83108936]\n",
      " [62277.21556652 60710.23343286]\n",
      " [59802.47889671 59078.8903438 ]]\n",
      "sigmoid ma aayeko input  [[ 23330.4008204 ]\n",
      " [108370.59957966]\n",
      " [ 49390.08576245]\n",
      " [ 68639.96602328]\n",
      " [120658.17282052]\n",
      " [114603.25902999]\n",
      " [ 99141.50115941]\n",
      " [ 50773.32907695]\n",
      " [ 78163.55339869]\n",
      " [ 75679.17051686]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      " y is  [[0 0 1 1 0 0 1 1 0 0]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 30]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 300   310\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[148 146 150 ...  64  65  66]\n",
      " [204 204 205 ...  98 102 103]\n",
      " [ 64  66  70 ...  31  40  40]\n",
      " ...\n",
      " [249 247 248 ... 169 172 163]\n",
      " [143 142 143 ... 155 156 157]\n",
      " [140 139 140 ...  32  32  32]] [[0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "within train hiden layer output [[54098.20501641 53667.38870319]\n",
      " [63799.93912198 61976.69198577]\n",
      " [29022.23764143 28896.28135598]\n",
      " [81110.22749024 78033.56746954]\n",
      " [61360.36341156 59747.92481179]\n",
      " [56073.18861841 55252.46301697]\n",
      " [48605.22938563 46548.92176706]\n",
      " [71878.5545683  69646.82001281]\n",
      " [69985.17580604 68899.00200579]\n",
      " [58872.31653801 57866.06089289]]\n",
      "sigmoid ma aayeko input  [[ 68638.57401544]\n",
      " [ 79901.25752509]\n",
      " [ 36906.51682729]\n",
      " [100976.33276433]\n",
      " [ 76958.28928897]\n",
      " [ 70846.60450418]\n",
      " [ 60340.82704428]\n",
      " [ 89877.31796442]\n",
      " [ 88374.70019217]\n",
      " [ 74268.15906199]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      " y is  [[0 1 0 0 0 1 1 0 0 1]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 31]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 310   320\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[103 106 114 ... 225 225 225]\n",
      " [129 131 129 ...  44  43  41]\n",
      " [255 255 255 ... 212 214 209]\n",
      " ...\n",
      " [ 89  90  92 ... 114  95  93]\n",
      " [102 103 104 ...  62  59  53]\n",
      " [ 70  50  22 ...  85 121  83]] [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "within train hiden layer output [[ 53604.25854222  51966.57368868]\n",
      " [ 58478.0655337   57927.54804931]\n",
      " [ 98237.17016638  96024.47471288]\n",
      " [ 65941.03273712  62958.22760216]\n",
      " [ 73317.12001365  71857.321081  ]\n",
      " [102104.58743799 101044.718009  ]\n",
      " [ 57358.31476947  56917.91248217]\n",
      " [ 57206.43825904  56695.67442009]\n",
      " [ 64506.26444812  62575.69011784]\n",
      " [ 37412.90491829  38162.50830303]]\n",
      "sigmoid ma aayeko input  [[ 67048.29500567]\n",
      " [ 74128.12728212]\n",
      " [123502.65858882]\n",
      " [ 81708.60030672]\n",
      " [ 92325.96668169]\n",
      " [129351.48320387]\n",
      " [ 72787.95068613]\n",
      " [ 72538.29646317]\n",
      " [ 80716.48908201]\n",
      " [ 48302.33992663]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      " y is  [[0 1 1 0 1 0 0 0 1 0]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 32]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 320   330\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[191 187 190 ...  90  90  99]\n",
      " [188 187 186 ... 152 151 148]\n",
      " [181 182 183 ... 166 166 166]\n",
      " ...\n",
      " [166 171 170 ... 216 217 232]\n",
      " [197 219 231 ...  72  71  68]\n",
      " [240 241 240 ... 193 195 200]] [[1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n",
      "within train hiden layer output [[62746.29389908 61836.125536  ]\n",
      " [87835.821228   86149.09002617]\n",
      " [78480.98943488 74684.27036092]\n",
      " [70885.14579098 69495.83825143]\n",
      " [82192.26507202 78467.0411452 ]\n",
      " [59054.95953115 57874.6601457 ]\n",
      " [64924.74003167 63852.88407713]\n",
      " [90183.71437642 87581.85782833]\n",
      " [33800.39971448 33883.16314733]\n",
      " [68034.08774958 67636.32763554]]\n",
      "sigmoid ma aayeko input  [[ 79284.38190314]\n",
      " [110658.34223643]\n",
      " [ 97050.72708147]\n",
      " [ 89281.0249618 ]\n",
      " [101839.91624447]\n",
      " [ 74362.54304501]\n",
      " [ 81933.47175678]\n",
      " [112923.90032769]\n",
      " [ 43165.31089134]\n",
      " [ 86434.69293749]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n",
      " y is  [[1 0 0 0 1 0 0 0 1 1]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 33]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 330   340\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[143 143 143 ... 155 149 166]\n",
      " [ 66  65  61 ...  69  78  77]\n",
      " [ 99  98  97 ... 104 103 104]\n",
      " ...\n",
      " [ 78  99 112 ... 212 214 214]\n",
      " [144 121  52 ... 201 202 171]\n",
      " [142 141 144 ...  98  96  95]] [[0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n",
      "within train hiden layer output [[71704.08223762 69035.16006982]\n",
      " [75030.28609797 72886.17353988]\n",
      " [61799.97571421 61089.0832219 ]\n",
      " [57103.44603142 55824.83041031]\n",
      " [49252.25812908 48422.992292  ]\n",
      " [75990.39347638 74890.19930164]\n",
      " [73834.60787012 70979.75900676]\n",
      " [59066.4294889  57911.22154589]\n",
      " [66505.18387205 65697.00148302]\n",
      " [69065.50450474 68825.73339765]]\n",
      "sigmoid ma aayeko input  [[89306.92652292]\n",
      " [93965.85963897]\n",
      " [78236.30652305]\n",
      " [71795.90325848]\n",
      " [62142.35723548]\n",
      " [96020.86816384]\n",
      " [91875.62256961]\n",
      " [74397.13662735]\n",
      " [84158.54968278]\n",
      " [87875.59463204]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n",
      " y is  [[0 0 0 1 1 0 1 0 1 1]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, mb 34]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 340   350\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[232 232 236 ... 225 218 214]\n",
      " [203 191 178 ...  44  64  80]\n",
      " [181 193 185 ...  35  39  39]\n",
      " ...\n",
      " [ 20  20  20 ... 195 195 195]\n",
      " [190 189 190 ...  57  93 118]\n",
      " [240 233 233 ... 215 195 181]] [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "within train hiden layer output [[103109.51992963 100012.14294758]\n",
      " [ 47074.02269301  46867.43679019]\n",
      " [ 53098.12565586  50537.15681476]\n",
      " [ 58101.3192389   56100.14027064]\n",
      " [ 90398.51804752  89080.48708647]\n",
      " [ 39099.09229967  38756.82137152]\n",
      " [ 78589.00620455  76293.08437303]\n",
      " [ 55264.22837129  55995.99927879]\n",
      " [ 67279.46367732  64537.52796574]\n",
      " [ 69064.06565485  66720.02182508]]\n",
      "sigmoid ma aayeko input  [[129011.40938652]\n",
      " [ 59860.39982759]\n",
      " [ 65668.15728042]\n",
      " [ 72493.2779978 ]\n",
      " [114219.48441068]\n",
      " [ 49583.48984569]\n",
      " [ 98382.81480078]\n",
      " [ 71050.58671384]\n",
      " [ 83606.9368242 ]\n",
      " [ 86199.15196193]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      " y is  [[0 1 1 0 1 1 1 0 1 0]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 35]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 350   360\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[220 218 219 ...  37  61  77]\n",
      " [243 239 238 ...  61  60  59]\n",
      " [ 19  28  31 ... 205 199 186]\n",
      " ...\n",
      " [218 220 225 ... 161 163 170]\n",
      " [ 57  62  58 ...  83  83  82]\n",
      " [117 113 114 ...  56  57  57]] [[0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "within train hiden layer output [[87547.54060069 85638.34798498]\n",
      " [57625.33193548 56633.97658885]\n",
      " [62139.27007634 61488.57045754]\n",
      " [67982.16146009 67419.6010544 ]\n",
      " [60174.15142399 58454.84861787]\n",
      " [64567.8172836  63358.25072324]\n",
      " [60022.42264189 59184.89048407]\n",
      " [90727.18924237 87472.85632645]\n",
      " [49888.63702401 49291.59945211]\n",
      " [50832.22006258 50450.95818497]]\n",
      "sigmoid ma aayeko input  [[110113.71440119]\n",
      " [ 72689.97179069]\n",
      " [ 78716.84714384]\n",
      " [ 86237.33085389]\n",
      " [ 75360.69211461]\n",
      " [ 81368.76333483]\n",
      " [ 75868.94561936]\n",
      " [113097.64239657]\n",
      " [ 63138.62196443]\n",
      " [ 64513.51323392]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      " y is  [[0 0 0 1 1 1 0 1 1 0]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 36]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 360   370\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[218 222 220 ... 224 225 228]\n",
      " [163 161 161 ... 115 114 113]\n",
      " [ 55  54  55 ...   4   4   6]\n",
      " ...\n",
      " [241 235 225 ... 128 127 129]\n",
      " [116 113 113 ...  55  54  48]\n",
      " [170 169 170 ... 173 168 167]] [[0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "within train hiden layer output [[82369.5205293  81384.55557062]\n",
      " [70373.650034   69417.27715415]\n",
      " [39234.5099875  37914.93730818]\n",
      " [81900.0391755  78694.52082456]\n",
      " [58230.13713652 57005.57313656]\n",
      " [80436.6062103  76134.00619818]\n",
      " [74975.58992987 73201.84765548]\n",
      " [74531.16816007 72832.96930058]\n",
      " [56997.48034823 56472.92940844]\n",
      " [79245.94431855 77436.36836581]]\n",
      "sigmoid ma aayeko input  [[104246.67103273]\n",
      " [ 88973.3456552 ]\n",
      " [ 48978.41295651]\n",
      " [101880.88402392]\n",
      " [ 73275.57568524]\n",
      " [ 99141.77092778]\n",
      " [ 94190.86144921]\n",
      " [ 93684.29232923]\n",
      " [ 72260.87975111]\n",
      " [ 99607.5249854 ]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      " y is  [[0 0 1 1 1 1 1 1 1 0]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 37]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 370   380\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[145 146 147 ... 145 111  45]\n",
      " [152 150 157 ... 119 116 102]\n",
      " [  2   2   2 ...  47  52  61]\n",
      " ...\n",
      " [176 174 176 ...  24  24  25]\n",
      " [119 118 117 ...  33  36  30]\n",
      " [109 108 108 ...  32  61  64]] [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "within train hiden layer output [[66345.58312986 65745.16852405]\n",
      " [58119.13306432 55612.37359099]\n",
      " [48281.95470422 47691.11543611]\n",
      " [82309.84028358 80739.85819473]\n",
      " [84230.54134653 82417.08978181]\n",
      " [17249.6215996  18281.80604891]\n",
      " [76350.64770009 73649.45374079]\n",
      " [67570.79830351 66819.74417193]\n",
      " [49918.1984291  49686.1230688 ]\n",
      " [56091.37285027 55090.22519823]]\n",
      "sigmoid ma aayeko input  [[ 84120.38519015]\n",
      " [ 72113.64499025]\n",
      " [ 61094.85462803]\n",
      " [103705.00768571]\n",
      " [105960.36058671]\n",
      " [ 22816.77561166]\n",
      " [ 95206.13593895]\n",
      " [ 85562.79093316]\n",
      " [ 63466.75780867]\n",
      " [ 70726.20919128]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      " y is  [[0 1 1 0 0 1 1 1 1 0]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 38]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 380   390\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[117 119 121 ...  85  85  86]\n",
      " [ 73  85 139 ... 123 124 124]\n",
      " [203 203 204 ... 156 155 155]\n",
      " ...\n",
      " [ 62  61  64 ... 177 174 176]\n",
      " [168 170 172 ... 252 253 251]\n",
      " [159 161 163 ... 117 118 119]] [[0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "within train hiden layer output [[67033.88928008 65550.53180166]\n",
      " [52300.93039343 49993.03744499]\n",
      " [71805.97016205 70509.27631742]\n",
      " [29400.24339339 29932.18524619]\n",
      " [65498.96500285 63738.19666643]\n",
      " [62859.59769402 63014.7963987 ]\n",
      " [45518.66459212 44986.88476861]\n",
      " [60947.36694495 59877.40374436]\n",
      " [57454.17313616 55108.73443718]\n",
      " [65888.38772659 64467.15819552]]\n",
      "sigmoid ma aayeko input  [[84295.39509564]\n",
      " [64853.04510626]\n",
      " [90528.88243666]\n",
      " [37912.0805385 ]\n",
      " [82117.42739582]\n",
      " [80276.69009853]\n",
      " [57618.35032122]\n",
      " [76863.38158165]\n",
      " [71394.12775485]\n",
      " [82884.19447128]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      " y is  [[0 1 0 0 0 0 1 1 1 0]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, mb 39]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 390   400\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[ 70  74  78 ...  87  86  85]\n",
      " [185 195 207 ... 170 174 178]\n",
      " [ 17  17  17 ...  13  14  16]\n",
      " ...\n",
      " [230 236 236 ...  34  29  30]\n",
      " [ 25  30  36 ...  17  20  20]\n",
      " [  1   0   0 ...  13  18  19]] [[0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "within train hiden layer output [[ 64316.57706104  64934.95078592]\n",
      " [ 88639.08040753  86252.14693861]\n",
      " [ 25515.20166184  25059.76813024]\n",
      " [106117.80181638 103193.35637272]\n",
      " [ 36368.38027397  35926.94073145]\n",
      " [ 58192.86993526  57363.8136573 ]\n",
      " [ 87900.51941918  86965.67060534]\n",
      " [ 69515.49209344  67357.10169562]\n",
      " [ 46704.43884132  47963.67223132]\n",
      " [ 23166.92806733  23474.4643208 ]]\n",
      "sigmoid ma aayeko input  [[ 82503.10287145]\n",
      " [111125.35752087]\n",
      " [ 32172.45056243]\n",
      " [132984.92714652]\n",
      " [ 46022.60627487]\n",
      " [ 73542.80608639]\n",
      " [111339.19759841]\n",
      " [ 86922.51671399]\n",
      " [ 60555.65731365]\n",
      " [ 29785.31995817]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      " y is  [[0 1 0 0 1 0 1 1 1 0]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 40]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 400   410\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[ 96  92  79 ... 112 113 114]\n",
      " [ 55  59  65 ...  69  71  75]\n",
      " [250 246 246 ... 221 220 223]\n",
      " ...\n",
      " [184 219 212 ...  52  53  54]\n",
      " [147 114 118 ...  76  87  97]\n",
      " [126 129 133 ... 185 186 187]] [[0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "within train hiden layer output [[ 61871.20416572  60271.84649824]\n",
      " [ 51664.7801193   51080.18009665]\n",
      " [ 98464.3328649   93702.27006886]\n",
      " [ 60822.68904746  59327.80153332]\n",
      " [ 58837.31500072  57905.71320496]\n",
      " [ 76948.11395647  75850.11746555]\n",
      " [116079.83613995 113817.69090522]\n",
      " [ 76177.84610895  74098.65708133]\n",
      " [ 38512.54884781  38480.30175911]\n",
      " [ 65561.40227983  64076.77171969]]\n",
      "sigmoid ma aayeko input  [[ 77620.07848189]\n",
      " [ 65413.30143963]\n",
      " [121763.49634712]\n",
      " [ 76366.24291807]\n",
      " [ 74282.93869666]\n",
      " [ 97243.8180592 ]\n",
      " [146214.70465209]\n",
      " [ 95480.79426841]\n",
      " [ 49082.30226587]\n",
      " [ 82416.79600464]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      " y is  [[0 0 1 1 0 1 0 0 1 0]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 41]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 410   420\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[ 50  51  51 ... 162 168 173]\n",
      " [253 247 245 ... 163 153 144]\n",
      " [210 218 225 ...  94 101 104]\n",
      " ...\n",
      " [166 171 178 ... 124 128 130]\n",
      " [198 182 181 ...  49  49  48]\n",
      " [255 254 255 ... 179 178 179]] [[1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "within train hiden layer output [[39207.07445633 38458.919231  ]\n",
      " [83231.53212899 82049.90472238]\n",
      " [70943.48352633 70712.26963661]\n",
      " [60101.10230127 59999.75486204]\n",
      " [78918.6345734  77347.97170216]\n",
      " [50676.99810341 50141.75855496]\n",
      " [73446.07319553 73100.2618183 ]\n",
      " [91218.84248048 88536.9142436 ]\n",
      " [70535.20693665 67189.27386782]\n",
      " [97926.44624458 94966.88125128]]\n",
      "sigmoid ma aayeko input  [[ 49398.17112281]\n",
      " [105189.32586432]\n",
      " [ 90277.03830735]\n",
      " [ 76555.13650802]\n",
      " [ 99380.29855683]\n",
      " [ 64193.05037883]\n",
      " [ 93376.92242445]\n",
      " [114180.0825678 ]\n",
      " [ 87277.70531061]\n",
      " [122512.0821374 ]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      " y is  [[1 0 0 1 0 0 1 0 0 1]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 42]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 420   430\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[159 167 166 ... 108 111 113]\n",
      " [ 47  55  44 ...  67  67  68]\n",
      " [ 75  96  74 ... 114 115 116]\n",
      " ...\n",
      " [ 84  84  90 ...  81  84  78]\n",
      " [136 136 136 ... 144 144 144]\n",
      " [168 172 173 ... 198 201 201]] [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "within train hiden layer output [[65639.29278121 64032.43305858]\n",
      " [38901.26164149 38106.27828607]\n",
      " [50315.66223931 48629.77205021]\n",
      " [69983.97402129 68128.50292126]\n",
      " [69948.3112355  71016.02352064]\n",
      " [75604.85941468 73041.02946766]\n",
      " [53599.45400222 53173.98454767]\n",
      " [50826.86039363 50496.4061026 ]\n",
      " [67984.08166618 66207.48357729]\n",
      " [82985.53185066 80897.95783608]]\n",
      "sigmoid ma aayeko input  [[ 82418.84354458]\n",
      " [ 48970.9590663 ]\n",
      " [ 62816.60370513]\n",
      " [ 87760.95084842]\n",
      " [ 90041.78467648]\n",
      " [ 94364.48838204]\n",
      " [ 68006.86567494]\n",
      " [ 64547.11240256]\n",
      " [ 85273.63213782]\n",
      " [104154.68368418]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      " y is  [[0 1 1 1 0 1 1 1 0 0]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 43]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 430   440\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[169 174 177 ...  33  25  40]\n",
      " [192 192 196 ...  79  80  81]\n",
      " [139 139 141 ...  34  39  42]\n",
      " ...\n",
      " [238 238 238 ... 166 157 167]\n",
      " [ 59  58  58 ...  43  53  56]\n",
      " [114 110 107 ...  72  76  75]] [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "within train hiden layer output [[ 66711.2655394   64607.4265519 ]\n",
      " [ 68980.36585029  68380.76718578]\n",
      " [ 47056.68396336  45571.5826128 ]\n",
      " [ 65730.76090659  64374.76631143]\n",
      " [ 76838.92803268  74897.60719654]\n",
      " [ 54521.34759121  53004.0900185 ]\n",
      " [ 78790.01480542  77215.60710641]\n",
      " [103430.08698986 101337.57301274]\n",
      " [ 28102.03782976  27862.05490515]\n",
      " [ 47709.63710196  47405.57206857]]\n",
      "sigmoid ma aayeko input  [[ 83390.2263026 ]\n",
      " [ 87480.67779148]\n",
      " [ 58820.83155491]\n",
      " [ 82735.11743165]\n",
      " [ 96433.46513701]\n",
      " [ 68313.5239446 ]\n",
      " [ 99213.31375415]\n",
      " [130219.85154352]\n",
      " [ 35642.41503233]\n",
      " [ 60593.30425887]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      " y is  [[0 1 1 1 0 0 1 1 0 0]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, mb 44]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 440   450\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[231 232 233 ...  68  68  74]\n",
      " [114 114 115 ...  79  78  79]\n",
      " [135 135 138 ...  37  35  30]\n",
      " ...\n",
      " [177 175 176 ...  52  48  45]\n",
      " [ 84  95 101 ...  41  48  49]\n",
      " [ 59  68  74 ... 136 143 150]] [[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "within train hiden layer output [[64796.49076569 63204.45182504]\n",
      " [59934.31883129 57218.44583012]\n",
      " [52773.39137877 51691.34066064]\n",
      " [70090.05672921 67876.39762071]\n",
      " [43393.50504801 42531.40329817]\n",
      " [71266.59823422 69225.06650078]\n",
      " [27979.28869371 27617.13630955]\n",
      " [54713.82301123 53745.78903744]\n",
      " [49561.50030737 49479.24025068]\n",
      " [66362.77668711 65159.57426777]]\n",
      "sigmoid ma aayeko input  [[81355.9710906 ]\n",
      " [74261.79497953]\n",
      " [66431.02722238]\n",
      " [87611.1674509 ]\n",
      " [54645.65917917]\n",
      " [89248.37096624]\n",
      " [35388.67206005]\n",
      " [68996.03105793]\n",
      " [63131.15271992]\n",
      " [83662.60241353]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      " y is  [[1 1 1 1 1 0 0 1 0 0]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 45]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 450   460\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[188 182 187 ... 181 169 157]\n",
      " [116 114 116 ...   8   7   7]\n",
      " [ 66  65  66 ...  27  27  27]\n",
      " ...\n",
      " [199 198 196 ... 121 126 135]\n",
      " [126 186 173 ... 146 139 126]\n",
      " [117 126 138 ... 135 143 144]] [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "within train hiden layer output [[77824.87057606 75863.5891613 ]\n",
      " [35480.19229742 35239.99771923]\n",
      " [50166.39383081 49223.74024028]\n",
      " [12381.21066926 11988.74935534]\n",
      " [71906.05650889 71556.78085974]\n",
      " [79221.80115445 78227.92465766]\n",
      " [80539.40996675 78577.60796735]\n",
      " [83367.54171674 81497.4974976 ]\n",
      " [70886.66318552 69370.63923772]\n",
      " [59897.53741981 59110.99671362]]\n",
      "sigmoid ma aayeko input  [[ 97674.7674887 ]\n",
      " [ 45050.22045601]\n",
      " [ 63217.74723878]\n",
      " [ 15475.25416263]\n",
      " [ 91410.47163679]\n",
      " [100225.88412645]\n",
      " [101135.69515846]\n",
      " [104814.90215668]\n",
      " [ 89182.1172216 ]\n",
      " [ 75750.28263405]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      " y is  [[0 1 1 0 1 1 0 1 1 0]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 46]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 460   470\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[200 194 187 ...  73  77  73]\n",
      " [207 204 202 ... 107 106 103]\n",
      " [ 53  57  57 ...  63  61  61]\n",
      " ...\n",
      " [ 71  70  71 ...  55  52  61]\n",
      " [ 84  85  81 ...  69  67  63]\n",
      " [136 137 138 ... 166 171 173]] [[1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "within train hiden layer output [[57837.58400772 56399.31951236]\n",
      " [60749.98982504 57678.7061589 ]\n",
      " [46195.31388232 45671.67507435]\n",
      " [65316.30060407 63478.68638918]\n",
      " [85130.36075618 83462.68120386]\n",
      " [51980.78112821 52292.9372785 ]\n",
      " [58627.39845202 58830.6468255 ]\n",
      " [44511.95761093 44859.12680354]\n",
      " [36376.9392647  35867.25998295]\n",
      " [54249.61703739 54104.86622754]]\n",
      "sigmoid ma aayeko input  [[ 72604.9607786 ]\n",
      " [ 75019.02642083]\n",
      " [ 58487.63272987]\n",
      " [ 81823.35485187]\n",
      " [107223.7384715 ]\n",
      " [ 66529.88852729]\n",
      " [ 74918.40029689]\n",
      " [ 57034.1653075 ]\n",
      " [ 45979.21386316]\n",
      " [ 69059.29601088]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      " y is  [[1 1 0 1 0 0 1 0 0 0]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 47]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 470   480\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[191 208 218 ... 163 176 188]\n",
      " [152 154 156 ...  87  80  83]\n",
      " [165 170 175 ... 209 198 192]\n",
      " ...\n",
      " [  1   2   2 ... 235 236 239]\n",
      " [189 185 184 ... 143 142 143]\n",
      " [ 85 124  93 ... 112 119 120]] [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "within train hiden layer output [[76426.03626885 74054.87951618]\n",
      " [59128.94697445 56980.94791888]\n",
      " [53150.63519741 52052.00990886]\n",
      " [59621.18104287 57562.68025184]\n",
      " [97644.92093742 96246.94466689]\n",
      " [97742.96515968 95501.22119936]\n",
      " [63593.46660189 61558.9756719 ]\n",
      " [63101.75741708 62886.03741902]\n",
      " [78105.14882437 74957.0146076 ]\n",
      " [57016.44290331 54678.25546017]]\n",
      "sigmoid ma aayeko input  [[ 95564.91277358]\n",
      " [ 73686.7771719 ]\n",
      " [ 66898.86510947]\n",
      " [ 74385.65467562]\n",
      " [123395.83871195]\n",
      " [122849.37463205]\n",
      " [ 79469.87132524]\n",
      " [ 80290.28889194]\n",
      " [ 97087.64056058]\n",
      " [ 70841.74329391]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      " y is  [[0 1 1 1 1 0 1 0 0 0]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 48]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 480   490\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[ 27  26  25 ...  37  32  33]\n",
      " [225 224 227 ...  61  63  62]\n",
      " [179 178 181 ... 149 151 151]\n",
      " ...\n",
      " [ 76  78  83 ...  89  96  92]\n",
      " [153 142 142 ... 168 168 166]\n",
      " [255 254 255 ... 188 188 191]] [[1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n",
      "within train hiden layer output [[33095.06322208 34166.93665972]\n",
      " [71576.48394844 69421.76843393]\n",
      " [64358.40330712 63466.26968638]\n",
      " [73366.24776061 70569.52461708]\n",
      " [64514.98103341 64453.68117057]\n",
      " [60311.51034748 59288.01894056]\n",
      " [91761.01764335 90378.44832056]\n",
      " [43758.43647931 42892.16336009]\n",
      " [52978.16230017 52055.91343562]\n",
      " [86236.99578858 82855.42799548]]\n",
      "sigmoid ma aayeko input  [[ 43053.07459437]\n",
      " [ 89553.43683744]\n",
      " [ 81354.35436289]\n",
      " [ 91324.66815221]\n",
      " [ 82215.19697069]\n",
      " [ 76089.57602169]\n",
      " [115905.46361974]\n",
      " [ 55107.66802339]\n",
      " [ 66819.30551897]\n",
      " [107270.8873073 ]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n",
      " y is  [[1 0 0 0 1 0 0 1 0 1]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, mb 49]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 490   500\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[211 210 212 ... 185 184 182]\n",
      " [248 249 249 ... 216 219 221]\n",
      " [133 135 138 ... 170 168 167]\n",
      " ...\n",
      " [165 169 172 ... 120 121 121]\n",
      " [139 139 143 ...  73  80  84]\n",
      " [226 227 229 ... 170 169 171]] [[1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "within train hiden layer output [[92888.84476998 91254.18413376]\n",
      " [97426.79935667 95195.3139554 ]\n",
      " [86663.38027182 84366.07310502]\n",
      " [88651.26689187 85785.36457809]\n",
      " [61520.84952309 59546.55965509]\n",
      " [69198.91906313 68072.73902326]\n",
      " [80050.92159601 78094.43309716]\n",
      " [82161.76936188 80781.74852211]\n",
      " [67755.86833264 66911.3430212 ]\n",
      " [92384.79775131 89971.34889981]]\n",
      "sigmoid ma aayeko input  [[117142.95201778]\n",
      " [122454.39169515]\n",
      " [108677.4442121 ]\n",
      " [110759.7269328 ]\n",
      " [ 76874.96105794]\n",
      " [ 87340.29723151]\n",
      " [100517.04629173]\n",
      " [103667.3743318 ]\n",
      " [ 85724.39036496]\n",
      " [115880.46564821]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      " y is  [[1 0 0 0 1 0 0 1 1 1]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 50]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 500   510\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[ 91  97  95 ...  14  19  22]\n",
      " [135 134 134 ...  77  76  75]\n",
      " [224 218 211 ... 171 166 163]\n",
      " ...\n",
      " [133 119 112 ...  52  48  47]\n",
      " [113 105 108 ... 164 167 169]\n",
      " [149 147 147 ... 168 169 168]] [[1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "within train hiden layer output [[63664.93279082 61290.95233536]\n",
      " [66987.62082255 65544.71792845]\n",
      " [77185.44142221 75442.45477681]\n",
      " [66349.50462685 65546.29182199]\n",
      " [67602.26885227 66539.76314744]\n",
      " [75876.51033164 75986.96482604]\n",
      " [55871.19381329 55299.00025383]\n",
      " [44763.5999256  43526.15277979]\n",
      " [77742.45787094 76086.03363737]\n",
      " [79937.55322632 77867.30397967]]\n",
      "sigmoid ma aayeko input  [[ 79290.82848561]\n",
      " [ 84268.59191825]\n",
      " [ 97033.14550831]\n",
      " [ 83963.99605784]\n",
      " [ 85355.06214178]\n",
      " [ 96839.10456269]\n",
      " [ 70786.82356451]\n",
      " [ 56093.97888824]\n",
      " [ 97812.29141719]\n",
      " [100281.95661159]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      " y is  [[1 0 0 1 1 1 0 0 0 0]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 51]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 510   520\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[134 118  93 ...  96  93  93]\n",
      " [126 115 109 ... 116 117 118]\n",
      " [137 136 137 ...  43  57  80]\n",
      " ...\n",
      " [100  97  97 ... 103 106 116]\n",
      " [255 240 231 ...  60  52  48]\n",
      " [191 189 190 ... 111 107  99]] [[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "within train hiden layer output [[68469.03889684 67208.69432598]\n",
      " [64290.10052684 62522.11245827]\n",
      " [49291.00607998 47838.71488992]\n",
      " [62775.18499945 60779.10948893]\n",
      " [43663.79406038 44136.78237268]\n",
      " [86729.65137241 85527.00065858]\n",
      " [99786.86165818 96450.36264013]\n",
      " [66412.17208285 65408.03196475]\n",
      " [51841.32384872 50326.73144206]\n",
      " [70369.41889215 68947.2860264 ]]\n",
      "sigmoid ma aayeko input  [[ 86302.84831486]\n",
      " [ 80570.24414617]\n",
      " [ 61695.95342579]\n",
      " [ 78457.04206967]\n",
      " [ 56052.790236  ]\n",
      " [109633.08932987]\n",
      " [124584.35123132]\n",
      " [ 83884.00348288]\n",
      " [ 64898.31428084]\n",
      " [ 88597.29253781]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      " y is  [[1 1 1 1 0 1 0 1 1 1]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 52]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 520   530\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[179 178 181 ... 137 134 130]\n",
      " [194 193 192 ... 188 187 188]\n",
      " [133 122 119 ...  95  95 100]\n",
      " ...\n",
      " [223 221 220 ... 193 193 193]\n",
      " [203 210 215 ...  99 101 100]\n",
      " [153 152 153 ... 151 150 151]] [[1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "within train hiden layer output [[73347.5796586  71197.46889688]\n",
      " [85201.7183127  82922.61881431]\n",
      " [65870.74309627 65466.94350174]\n",
      " [52274.70441078 50333.39232167]\n",
      " [65017.04065806 63607.82598883]\n",
      " [95518.33006471 94062.69518604]\n",
      " [91187.59366979 89516.25786579]\n",
      " [83834.63071614 81443.98099426]\n",
      " [64722.52155544 62660.99978034]\n",
      " [64260.66430253 61813.04999118]]\n",
      "sigmoid ma aayeko input  [[ 91815.44688822]\n",
      " [106828.15129955]\n",
      " [ 83671.37974728]\n",
      " [ 65111.33391991]\n",
      " [ 81782.69088726]\n",
      " [120638.3115289 ]\n",
      " [114944.47961356]\n",
      " [104996.18825769]\n",
      " [ 80888.03142809]\n",
      " [ 79991.85413696]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      " y is  [[1 0 1 1 0 0 0 0 1 0]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 53]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 530   540\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[198 197 199 ... 129 132 133]\n",
      " [209 220 247 ... 126 128 131]\n",
      " [ 33  30  31 ... 174 196 220]\n",
      " ...\n",
      " [ 25  26  28 ...  35  36  35]\n",
      " [218 217 219 ... 226 225 225]\n",
      " [174 174 176 ... 133 133 133]] [[1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "within train hiden layer output [[ 60287.49851781  58196.23403434]\n",
      " [ 80871.47548988  79031.32195668]\n",
      " [ 44733.37505934  44130.38421312]\n",
      " [ 66505.77909612  65016.92099757]\n",
      " [ 78650.16571316  75145.20696321]\n",
      " [ 62641.92158312  62186.27777615]\n",
      " [ 42921.00494266  41253.81846877]\n",
      " [ 22645.38132924  22956.50586551]\n",
      " [102154.62592998  99783.47149461]\n",
      " [ 78323.8384783   75789.03432087]]\n",
      "sigmoid ma aayeko input  [[ 75209.21035598]\n",
      " [101655.92537023]\n",
      " [ 56560.34748409]\n",
      " [ 83617.61795658]\n",
      " [ 97498.63250926]\n",
      " [ 79513.01213681]\n",
      " [ 53402.47155137]\n",
      " [ 29123.14501699]\n",
      " [128371.74999335]\n",
      " [ 97854.59052651]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      " y is  [[1 0 1 1 0 1 0 0 0 1]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, mb 54]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 540   550\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[ 86  88  88 ...  57  58  59]\n",
      " [143 144 148 ...  80  77  79]\n",
      " [ 90  93  95 ... 173 174 174]\n",
      " ...\n",
      " [ 28  29  31 ... 117 112 108]\n",
      " [ 11  11  10 ... 126  82  35]\n",
      " [212 208 208 ...  97  98  98]] [[1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "within train hiden layer output [[49102.96522443 48039.58602092]\n",
      " [63753.37482832 61225.97907225]\n",
      " [65517.82525847 63468.80369738]\n",
      " [63372.30068737 61065.94469958]\n",
      " [98285.51227023 96175.11162482]\n",
      " [26238.96476796 25465.05637403]\n",
      " [62274.80836295 60910.726032  ]\n",
      " [44345.85412256 43780.16584014]\n",
      " [41706.76709329 41971.95146959]\n",
      " [92968.85924806 91760.96998205]]\n",
      "sigmoid ma aayeko input  [[ 61765.68154637]\n",
      " [ 79281.51207682]\n",
      " [ 81912.08061903]\n",
      " [ 78971.5061996 ]\n",
      " [123645.70774614]\n",
      " [ 32841.88314503]\n",
      " [ 78321.95428236]\n",
      " [ 56095.90072997]\n",
      " [ 53391.98840525]\n",
      " [117584.60972796]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      " y is  [[1 1 0 1 0 0 1 1 0 0]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 55]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 550   560\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[ 46  46  48 ...  62  78 125]\n",
      " [249 249 250 ... 233 234 236]\n",
      " [196 193 194 ... 239 238 236]\n",
      " ...\n",
      " [239 240 240 ...  16  16  16]\n",
      " [160 160 159 ...  77  78  79]\n",
      " [147 152 157 ...  14  11  10]] [[0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "within train hiden layer output [[ 48357.46908152  49506.89474865]\n",
      " [ 98401.04290153  94997.24087442]\n",
      " [101350.42652968  99001.10458583]\n",
      " [ 75083.87880256  73346.48716024]\n",
      " [ 92976.19788933  91318.32292719]\n",
      " [ 78718.39018441  75661.05955108]\n",
      " [ 31800.91360672  32492.25399289]\n",
      " [ 63817.56601181  62593.67288185]\n",
      " [ 60142.1748136   59448.24286122]\n",
      " [ 46399.99397898  44276.91903197]]\n",
      "sigmoid ma aayeko input  [[ 62576.06970138]\n",
      " [122763.71637763]\n",
      " [127363.68021109]\n",
      " [ 94357.87021072]\n",
      " [117235.86276851]\n",
      " [ 97941.85471312]\n",
      " [ 41100.05557239]\n",
      " [ 80400.70760517]\n",
      " [ 76135.92199634]\n",
      " [ 57475.77042659]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      " y is  [[0 0 0 1 1 1 1 1 1 1]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 56]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 560   570\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[115 109  85 ... 157 157 156]\n",
      " [149 155 157 ...  72  71  67]\n",
      " [104 101 105 ... 151 150 150]\n",
      " ...\n",
      " [ 78  81  83 ...  56  57  57]\n",
      " [143 144 144 ... 144 144 144]\n",
      " [160 161 162 ...  68  62  49]] [[0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n",
      "within train hiden layer output [[61457.26524732 59489.79428544]\n",
      " [65244.46327896 64226.46769611]\n",
      " [69602.24751157 68142.26218451]\n",
      " [69894.82593203 69035.51792812]\n",
      " [85111.19434996 83757.42489445]\n",
      " [74600.23092079 73150.77796001]\n",
      " [55003.68686325 53681.6712849 ]\n",
      " [48262.49135918 47223.46871523]\n",
      " [70321.49387502 69046.62329984]\n",
      " [42228.72456676 41983.7463611 ]]\n",
      "sigmoid ma aayeko input  [[ 76799.31055545]\n",
      " [ 82384.01752426]\n",
      " [ 87588.93955832]\n",
      " [ 88440.03676846]\n",
      " [107449.11294908]\n",
      " [ 93970.31035506]\n",
      " [ 69083.93672828]\n",
      " [ 60713.36644243]\n",
      " [ 88653.37604206]\n",
      " [ 53651.54857418]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n",
      " y is  [[0 1 0 1 1 1 0 1 0 1]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 57]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 570   580\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[ 70  66  65 ...  64  57  60]\n",
      " [109 113 120 ...  72  73  70]\n",
      " [144 145 144 ...  29  34  38]\n",
      " ...\n",
      " [180 179 181 ...  89  83  78]\n",
      " [138 140 144 ... 104 106 105]\n",
      " [249 245 245 ...  34  34  35]] [[1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "within train hiden layer output [[42098.95752702 41361.35816827]\n",
      " [54370.37451137 53851.50004382]\n",
      " [55537.70417577 53838.2010112 ]\n",
      " [75666.19069557 73756.54100373]\n",
      " [75018.82277839 72609.38554285]\n",
      " [73452.26651219 71513.79593971]\n",
      " [83408.67836004 80869.18482536]\n",
      " [54058.54456602 52328.74919414]\n",
      " [73681.87273064 71989.58387494]\n",
      " [85321.08089326 83098.06114336]]\n",
      "sigmoid ma aayeko input  [[ 53094.04664644]\n",
      " [ 68915.54278501]\n",
      " [ 69464.45905439]\n",
      " [ 94963.29791066]\n",
      " [ 93740.09374541]\n",
      " [ 92117.36041227]\n",
      " [104334.59939605]\n",
      " [ 67554.25783571]\n",
      " [ 92606.04872999]\n",
      " [107024.98097064]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      " y is  [[1 1 1 0 1 0 0 1 0 0]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 58]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 580   590\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[148 127 133 ... 220 222 218]\n",
      " [ 73  65  52 ... 153 124 112]\n",
      " [ 95  94  93 ...  32  28  20]\n",
      " ...\n",
      " [117 116 116 ...  74  69  64]\n",
      " [214 217 220 ... 137 140 137]\n",
      " [ 96 105 101 ...  33  33  37]] [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "within train hiden layer output [[65439.97052392 64328.20547976]\n",
      " [53379.38976494 54110.2196284 ]\n",
      " [44242.23142984 44821.20158125]\n",
      " [78165.44003878 76400.39922445]\n",
      " [67421.36192649 64671.33506924]\n",
      " [84657.9347934  83169.92794054]\n",
      " [53129.22104615 51544.74461052]\n",
      " [57057.03024587 56564.97815954]\n",
      " [88313.19966489 86175.52461688]\n",
      " [54978.76655605 54744.97103056]]\n",
      "sigmoid ma aayeko input  [[ 82558.6881997 ]\n",
      " [ 68646.4573503 ]\n",
      " [ 56874.70445584]\n",
      " [ 98265.20258038]\n",
      " [ 83781.43395488]\n",
      " [106764.32832436]\n",
      " [ 66484.91291834]\n",
      " [ 72362.67556076]\n",
      " [110908.18618815]\n",
      " [ 69918.18040868]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      " y is  [[0 1 1 1 0 0 0 1 1 1]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, mb 59]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 590   600\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[ 35  33  34 ...  44  35  48]\n",
      " [151 148 149 ... 123 124 149]\n",
      " [211 222 229 ...  92  95 108]\n",
      " ...\n",
      " [ 24  24  25 ...  18  19  21]\n",
      " [ 86  85  84 ...  86  96 105]\n",
      " [ 46  46  41 ...  41  42  43]] [[1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "within train hiden layer output [[40226.08272872 40487.80542796]\n",
      " [76779.21509342 74353.81613958]\n",
      " [80801.90468054 79760.03316905]\n",
      " [59433.04015956 58506.73650005]\n",
      " [49854.09545965 49656.75460863]\n",
      " [73383.28484119 71796.28108634]\n",
      " [61838.2418657  59884.18835061]\n",
      " [36395.827025   36817.83630794]\n",
      " [53299.34719351 51739.40808111]\n",
      " [27704.31313724 27921.78841091]]\n",
      "sigmoid ma aayeko input  [[ 51501.19504299]\n",
      " [ 95972.08869896]\n",
      " [102202.49814846]\n",
      " [ 75046.77136736]\n",
      " [ 63412.66151863]\n",
      " [ 92309.10297773]\n",
      " [ 77295.77622328]\n",
      " [ 46744.74791747]\n",
      " [ 66721.36994861]\n",
      " [ 35499.32273925]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      " y is  [[1 0 1 1 0 0 0 1 1 0]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 60]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 600   610\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[152 154 156 ... 111 113 117]\n",
      " [ 66  66  66 ...  76  78  77]\n",
      " [121 121 120 ...  50  51  43]\n",
      " ...\n",
      " [168 164 142 ... 147 147 147]\n",
      " [255 255 255 ... 196 197 199]\n",
      " [ 62  63  63 ...  81 107 143]] [[0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "within train hiden layer output [[ 62592.76078012  61639.26715303]\n",
      " [ 33611.16659208  33221.47602963]\n",
      " [ 45606.22958204  45891.20663818]\n",
      " [ 80363.64836909  78496.84236655]\n",
      " [ 64270.92906111  62998.80024404]\n",
      " [ 65175.81279628  63952.01537118]\n",
      " [ 57158.76187604  54720.68292869]\n",
      " [ 63969.69252774  62017.56368752]\n",
      " [102954.79637406 100494.94370294]\n",
      " [ 44493.6142206   43667.04653092]]\n",
      "sigmoid ma aayeko input  [[ 79054.13115278]\n",
      " [ 42548.03233354]\n",
      " [ 58379.99139017]\n",
      " [100987.17848951]\n",
      " [ 80940.40970841]\n",
      " [ 82132.70064544]\n",
      " [ 70943.72095867]\n",
      " [ 80015.1463835 ]\n",
      " [129321.46976406]\n",
      " [ 56076.70014363]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      " y is  [[0 0 0 1 1 1 0 0 0 1]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 61]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 610   620\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[148 148 146 ...  91 102 105]\n",
      " [100  99 101 ...  35  33  30]\n",
      " [ 84  84  84 ...  86  86  88]\n",
      " ...\n",
      " [165 165 164 ...  72  70  74]\n",
      " [ 43  43  42 ...  41  36  34]\n",
      " [147 145 145 ... 150 150 150]] [[1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "within train hiden layer output [[70486.9277261  69478.39001042]\n",
      " [33645.92198616 33342.19109462]\n",
      " [45377.10346721 44456.04659184]\n",
      " [48689.07712571 47499.01284039]\n",
      " [66884.82270656 66390.33093594]\n",
      " [65795.4727004  65035.04985021]\n",
      " [58309.84880189 57790.9642031 ]\n",
      " [58787.34265757 56605.69997941]\n",
      " [46519.67591987 46800.82996087]\n",
      " [79881.14519402 78210.84014677]]\n",
      "sigmoid ma aayeko input  [[ 89076.27394425]\n",
      " [ 42660.75719563]\n",
      " [ 57128.0522113 ]\n",
      " [ 61137.11017527]\n",
      " [ 84892.27087499]\n",
      " [ 83291.596068  ]\n",
      " [ 73938.80596341]\n",
      " [ 73224.41934381]\n",
      " [ 59541.69544047]\n",
      " [100528.31091512]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      " y is  [[1 0 0 0 0 1 1 0 1 0]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 62]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 620   630\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[131 127 117 ... 127 128 127]\n",
      " [110 104 102 ...  92  93  83]\n",
      " [151 189 191 ... 162 162 162]\n",
      " ...\n",
      " [ 30  30  31 ...  35  36  36]\n",
      " [ 97  95  95 ...  94  98 100]\n",
      " [251 249 250 ... 106 113 129]] [[1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "within train hiden layer output [[50658.42929436 50373.54371673]\n",
      " [59128.66043542 58778.60938279]\n",
      " [96623.2618083  93337.53958008]\n",
      " [88298.62478742 85950.59874027]\n",
      " [85195.7961929  85664.17579748]\n",
      " [51595.20028587 51318.94926026]\n",
      " [68818.7567864  67360.74008589]\n",
      " [26412.03818684 25951.28211496]\n",
      " [53884.62356038 54112.38699266]\n",
      " [81580.77607035 78498.45898876]]\n",
      "sigmoid ma aayeko input  [[ 64368.6080568 ]\n",
      " [ 75117.24252722]\n",
      " [120590.81569786]\n",
      " [110722.20148752]\n",
      " [109007.08002572]\n",
      " [ 65569.96755086]\n",
      " [ 86591.46773218]\n",
      " [ 33311.78189794]\n",
      " [ 68890.34016576]\n",
      " [101571.83297754]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      " y is  [[1 0 0 0 1 1 0 0 0 1]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n",
      "[Epoch 1, mb 63]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 630   640\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[141 143 149 ...  24  20  19]\n",
      " [109 118 131 ...  48  45  48]\n",
      " [225 221 221 ... 230 228 227]\n",
      " ...\n",
      " [158 154 157 ... 107 107 124]\n",
      " [122 121 123 ...  38  37  38]\n",
      " [127 125 125 ... 172 175 173]] [[1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "within train hiden layer output [[61532.65384238 60689.30222205]\n",
      " [53010.77620387 52333.95672556]\n",
      " [96380.68222627 94639.67213508]\n",
      " [62406.36095025 62083.25960767]\n",
      " [33333.35717958 32301.73819056]\n",
      " [70875.23635555 68870.24252576]\n",
      " [77901.10820561 75546.92125097]\n",
      " [68833.40902908 67350.4067042 ]\n",
      " [47273.32318537 45757.15521477]\n",
      " [64680.612103   63856.73747826]]\n",
      "sigmoid ma aayeko input  [[ 77790.02845569]\n",
      " [ 67056.20784271]\n",
      " [121510.8018274 ]\n",
      " [ 79318.1249698 ]\n",
      " [ 41682.93909185]\n",
      " [ 88778.41805409]\n",
      " [ 97459.29960213]\n",
      " [ 86590.26709447]\n",
      " [ 59072.34761722]\n",
      " [ 81819.52809846]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      " y is  [[1 1 0 1 0 0 1 1 1 1]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " a and b are  [[0.]] [[nan]]\n",
      "loss is   [[nan]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, mb 64]    Avg.Loss = 0.000num of examples num of batches  10000   1000\n",
      "batch start and end 640   650\n",
      "Chosen learning rate, inertia of momentum and l2 penalty factor are\n",
      "0.003 0.003 3e-07\n",
      "train size is   (10000, 1000)\n",
      "within train x_batch, y_batch [[ 80  80  77 ... 126 132 134]\n",
      " [121 106  89 ...   4   5   2]\n",
      " [ 76  76  60 ...  42  42  45]\n",
      " ...\n",
      " [216 211 182 ... 131 137 140]\n",
      " [ 98 114 115 ... 176 179 185]\n",
      " [212 206 208 ... 151 142 135]] [[1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "within train hiden layer output [[65859.85134729 64538.65428846]\n",
      " [38936.36596516 37836.93132856]\n",
      " [40336.29976823 40846.57301277]\n",
      " [74314.32391322 70950.43627447]\n",
      " [96874.68020323 94175.99802534]\n",
      " [82537.05651698 80711.10488926]\n",
      " [36149.33898667 36670.31827552]\n",
      " [66167.07646332 63368.6067924 ]\n",
      " [62150.19730537 60570.95123179]\n",
      " [89568.85504883 87637.22210256]]\n",
      "sigmoid ma aayeko input  [[ 82927.41454966]\n",
      " [ 48773.43466991]\n",
      " [ 51839.53392187]\n",
      " [ 92082.21452064]\n",
      " [121378.57654089]\n",
      " [103791.02991507]\n",
      " [ 46509.20967872]\n",
      " [ 82143.52798424]\n",
      " [ 77991.83107495]\n",
      " [112673.25855252]]\n",
      "sigmoid is  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "within train sigmoid  output [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y_cap_for_batch [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y cap and y for loss  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] [[1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      " y is  [[1 1 1 0 0 0 0 0 0 1]]\n",
      "y cap is [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "log id   [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-34ae0a7d93b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    342\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y_cap_for_batch\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_cap_for_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 344\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_cap_for_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_start\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_end\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    345\u001b[0m             \u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-34ae0a7d93b7>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, y_cap, y)\u001b[0m\n\u001b[0;32m    209\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"log id  \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_cap_log\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_cap\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m         \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0my_cap\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" a and b are \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Aashish Adhikari\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "try:\n",
    "   import _pickle as pickle\n",
    "except:\n",
    "   import pickle\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "# This is a class for a LinearTransform layer which takes an input \n",
    "# weight matrix W and computes W x as the forward step\n",
    "\n",
    "\n",
    "\n",
    "class LinearTransform(object):\n",
    "\n",
    "    def __init__(self, W, b):\n",
    "        self.weights = W #weights for all n nodes in the hidden layer\n",
    "        self.bias = b\n",
    "        self.relu_object = ReLU()\n",
    "        self.sig = SigmoidCrossEntropy()\n",
    "    def forward(self, x,y):\n",
    "        #print(\"aa\",x.shape,self.weights.shape)\n",
    "        \n",
    "        \n",
    "        #Never do this because it does element-wise and this gives an error\n",
    "        #batch_linear_summation = x * self.weights# + self.bias\n",
    "        batch_linear_summation = np.dot(x, self.weights) + self.bias\n",
    "        #print(\"batch_linear_summation is \",batch_linear_summation)\n",
    "        return self.relu_object.forward(batch_linear_summation,y)\n",
    "        \n",
    "    def forward_2(self,x):\n",
    "        batch_linear_summation = np.dot(x,self.weights)+self.bias\n",
    "        return self.sig.forward(batch_linear_summation)\n",
    "        print()\n",
    "    def backward(\n",
    "        self, \n",
    "        grad_output, \n",
    "        learning_rate=0.0, \n",
    "        momentum=0.0, \n",
    "        l2_penalty=0.0,\n",
    "    ):\n",
    "        print()\n",
    "\t# DEFINE backward function\n",
    "# ADD other operations in LinearTransform if needed\n",
    "\n",
    "\n",
    "\n",
    "# This is a class for a ReLU layer max(x,0)\n",
    "\n",
    "\n",
    "\n",
    "class ReLU(object):\n",
    "    def __init__(self):\n",
    "        print()\n",
    "        \n",
    "    def forward(self, x,y):\n",
    "\t# DEFINE forward function\n",
    "        relu_output = np.maximum(0,x)\n",
    "        #print(\"relu_output is\",relu_output)\n",
    "        return relu_output\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def backward(\n",
    "        self, \n",
    "        grad_output, \n",
    "        learning_rate=0.0, \n",
    "        momentum=0.0, \n",
    "        l2_penalty=0.0,\n",
    "    ):\n",
    "        print()\n",
    "    # DEFINE backward function\n",
    "# ADD other operations in ReLU if needed\n",
    "\n",
    "\n",
    "\n",
    "# This is a class for a sigmoid layer followed by a cross entropy layer, the reason \n",
    "# this is put into a single layer is because it has a simple gradient form\n",
    "\n",
    "\n",
    "\n",
    "class SigmoidCrossEntropy(object):\n",
    "    def __init__(self):\n",
    "        print()\n",
    "        #self.linear_transform_object_2 = LinearTransform(self.second_layer_weights,self.second_layer_bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         linear_summation = self.linear_transform_object_2.forward(self,x)\n",
    "#         print(\"linear summation size \",linear_summation.shape)\n",
    "#         #linear transform gar paile\n",
    "        #ani balla sigmoid\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(\"sigmoid ma aayeko input \", x)\n",
    "        \n",
    "        sigmoid =  (1 / (1 + np.exp(-x)))\n",
    "        \n",
    "        \n",
    "        #element-wise comparison\n",
    "        \n",
    "#         sigmoid = np.where(sigmoid<0.5,0,1)\n",
    "#         if (sigmoid < 0.5):\n",
    "#             sigmoid = 0\n",
    "#         else:\n",
    "#             sigmoid = 1\n",
    "            \n",
    "        print(\"sigmoid is \", sigmoid)\n",
    "        return sigmoid\n",
    "        \n",
    "    def backward(self,grad_output,learning_rate=0.0,momentum=0.0,l2_penalty=0.0):\n",
    "        print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# This is a class for the Multilayer perceptron\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MLP(object):\n",
    "\n",
    "    def __init__(self, input_dims, hidden_units):\n",
    "    # INSERT CODE for initializing the network\n",
    "        self.input_dims = input_dims\n",
    "        self.hidden_units = hidden_units\n",
    "        self.first_Layer_Weights = np.random.rand(input_dims, hidden_units) #% (0.2 + 1 - 0) + 0\n",
    "        self.first_layer_biases = np.random.rand(hidden_units) #%  (0.2 + 1 - 0) + 0\n",
    "        self.second_layer_weights = np.random.rand(hidden_units,1) #%  (0.2 + 1 - 0) + 0\n",
    "        self.second_layer_bias = np.random.rand(1) #%  (0.2 + 1 - 0) + 0\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #check\n",
    "#         self.first_Layer_Weights =np.array([[1,2],[1,3]])\n",
    "        \n",
    "#         self.first_layer_biases = np.array([[1],[1]])\n",
    "        \n",
    "#         self.second_layer_weights = np.array([[1,1]])\n",
    "#         self.second_layer_bias = np.array([[1]])\n",
    "        \n",
    "        \n",
    "        #print(self.first_Layer_Weights.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.linear_transform_object = LinearTransform(self.first_Layer_Weights,self.first_layer_biases)#remember the dilemma you got here,\n",
    "        #aafule pass garnu parne bhayepaxi jasati ni afnai attributes hunxa ni ta, so u never face that dilemma\n",
    "        self.linear_transform_object_for_sigmoid = LinearTransform(self.second_layer_weights,self.second_layer_bias)\n",
    "\n",
    "    def train(self,x_batch,y_batch,learning_rate,momentum,l2_penalty):\n",
    "        print(\"within train x_batch, y_batch\",x_batch,y_batch)\n",
    "        #print(\"x batch and y batch \",len(x_batch),len(y_batch))\n",
    "        hidden_layer_output = self.linear_transform_object.forward(x_batch,y_batch)\n",
    "        print(\"within train hiden layer output\",hidden_layer_output)\n",
    "        sigmoid_output = self.linear_transform_object_for_sigmoid.forward_2(hidden_layer_output)\n",
    "        print(\"within train sigmoid  output\",sigmoid_output)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return sigmoid_output\n",
    "        \n",
    "        #yeha bata linear transform ma pathai\n",
    "            \n",
    "            \n",
    "        #10 ota palai palo ayepaxi tiotal cumulative ma kaam garne yeha\n",
    "        \n",
    "            \n",
    "        \n",
    "        ######Yeha k garne bhar aaba\n",
    "        \n",
    "        \n",
    "\t# INSERT CODE for training the network\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def evaluate(self, y_cap, y):\n",
    "        print(\"y cap and y for loss \", y_cap, y)\n",
    "        y = np.transpose(y)\n",
    "#         loss = \n",
    "\n",
    "        print (\" y is \",y)\n",
    "        print(\"y cap is\",y_cap)\n",
    "        \n",
    "        #if (y_cap)\n",
    "        \n",
    "#         a = np.where(sigmoid<0.5,0,1)\n",
    "# #         if (sigmoid < 0.5):\n",
    "# #             sigmoid = 0\n",
    "# #         else:\n",
    "# #             sigmoid = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------figure out a way to avoid this\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        y_cap_log = np.log(y_cap)\n",
    "        print(\"log id  \",y_cap_log)\n",
    "        \n",
    "        a = np.dot(y,np.log(y_cap))\n",
    "        b = np.dot((1-y),(np.log(1-y_cap)))\n",
    "        print(\" a and b are \",a,b)\n",
    "        loss = a + b\n",
    "        #now the loss is 1-dimensional\n",
    "        print(\"loss is  \",loss) \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #implement loss now\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         loss = x - y\n",
    "#         print(loss)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\t# INSERT CODE for testing the network\n",
    "# ADD other operations and data entries in MLP if needed\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if sys.version_info[0] < 3:\n",
    "        print(\"system version is less than 3\")\n",
    "        #train_x, train_y, test_x, test_y = \n",
    "        data = pickle.load(open('dataset_folder/cifar_2class_py2.p', 'rb'))\n",
    "        \n",
    "    else:\n",
    "\t    #train_x, train_y, test_x, test_y \n",
    "        data = pickle.load(open('../../dataset_folder/cifar_2class_py2.p', 'rb'), encoding='bytes')\n",
    "        \n",
    "    #print(data)\n",
    "    \n",
    "    #print(data[b'test_data'])\n",
    "    train_x = np.array(data[b'train_data'])\n",
    "    \n",
    "    train_y = np.array(data[b'train_labels'])\n",
    "    \n",
    "    test_x = np.array(data[b'test_data'])\n",
    "    test_y = np.array(data[b'test_labels'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    def normalize(x):\n",
    "         \n",
    "        top = x - np.amin(x)\n",
    "        bottom = np.amax(x)-np.amin(x)\n",
    "        \n",
    "        return(top/bottom)\n",
    "    \n",
    "    \n",
    "    #for checking------------------remove paxi\n",
    "    print(\"dtype is \",train_x.dtype)\n",
    "    train_x = train_x[...,0:1000]\n",
    "    train_y = train_y[...,0:1000]\n",
    "    test_x = test_x[...,0:1000]\n",
    "    test_y = test_y[...,0:1000]\n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(\" minimum in x is \", np.amax(train_x))\n",
    "    #train_x = normalize(train_x)\n",
    "    print(\" normalized x is \",train_x)\n",
    "    \n",
    "    \n",
    "#     print(test_y)\n",
    "\t\n",
    "    num_examples, input_dims = train_x.shape\n",
    "\t# INSERT YOUR CODE HERE\n",
    "    #ask the user about the the number of hidden nodes hs/ she wants\n",
    "    \n",
    "    #inp = input(\"How many hidden nodes do you want?\\n\")\n",
    "    #num_of_hidden_nodes = int(inp)\n",
    "    num_of_hidden_nodes = 2\n",
    "    print(\"dimension of each example is \",input_dims)\n",
    "    \n",
    "    \n",
    "\t\n",
    "    \n",
    "    \n",
    "    # YOU CAN CHANGE num_epochs AND num_batches TO YOUR DESIRED VALUES\n",
    "    num_epochs = 10\n",
    "    num_batches = 1000\n",
    "    learning_rate = [0.001,0.003,0.01,0.03,0.1,0.3,1,3,10]\n",
    "    inertia_of_momentum = [0.001,0.003,0.01,0.03,0.1,0.3,1,3,10]\n",
    "    l2_penalty_factor = [0.0000001,0.0000003,0.000001,0.000003,0.00001,0.00003,0.0001,0.0003,0.001,0.003,0.01,0.03,1,3,10]\n",
    "    print(\"Choose the corresponding index number for the learning rate you want to use\")\n",
    "    #lr = int(input(\"[0.001,0.003,0.01,0.03,0.1,0.3,1,3,10]\"))\n",
    "    lr =1 \n",
    "    print(\"Choose the corresponding index number for the inertia of momentum you want to use\")\n",
    "    #iner = int(input(\"[0.001,0.003,0.01,0.03,0.1,0.3,1,3,10]\"))\n",
    "    \n",
    "    iner = 1\n",
    "    print(\"Choose the corresponding index number for the L2 penalty factor you want to use\")\n",
    "    #penalty = int(input(\"[0.0000001,0.0000003,0.000001,0.000003,0.00001,0.00003,0.0001,0.0003,0.001,0.003,0.01,0.03,1,3,10]\"))\n",
    "    penalty = 1\n",
    "    mlp = MLP(input_dims, num_of_hidden_nodes)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "\t# INSERT YOUR CODE FOR EACH EPOCH HERE\n",
    "\n",
    "        for b in range(num_batches):\n",
    "            total_loss = 0.0\n",
    "            print(\"num of examples num of batches \",num_examples, \" \",num_batches)\n",
    "            \n",
    "            batch_start = int( (num_examples / num_batches) * b)\n",
    "            batch_end = int((num_examples / num_batches)*(b +1))\n",
    "            print(\"batch start and end\",batch_start,\" \",batch_end)\n",
    "            print(\"Chosen learning rate, inertia of momentum and l2 penalty factor are\")\n",
    "            print(learning_rate[lr], inertia_of_momentum[iner], l2_penalty_factor[penalty])\n",
    "            print(\"train size is  \",train_x.shape)\n",
    "            y_cap_for_batch = mlp.train(train_x[batch_start:batch_end,...],train_y[batch_start:batch_end,...],int(learning_rate[lr]), inertia_of_momentum[iner], l2_penalty_factor[penalty])\n",
    "            print(\"y_cap_for_batch\",y_cap_for_batch)\n",
    "            \n",
    "            loss = mlp.evaluate(y_cap_for_batch,train_y[batch_start:batch_end,...] )\n",
    "            exit()\n",
    "            \n",
    "            \n",
    "            ####mlp lai cll garda aba kata kata k hunxa network ma layer haru ma kaam gar ani tespaxi directly tesko loss return garne \n",
    "            #####actual loss function ma k garne \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # INSERT YOUR CODE FOR EACH MINI_BATCH HERE\n",
    "\t\t\t# MAKE SURE TO UPDATE total_loss\n",
    "            print(\n",
    "                '\\r[Epoch {}, mb {}]    Avg.Loss = {:.3f}'.format(\n",
    "                    epoch + 1,\n",
    "                    b + 1,\n",
    "                    total_loss,\n",
    "                ),\n",
    "                end='',\n",
    "            )\n",
    "            \n",
    "            #after each mini bach update you want to update the momentum value\n",
    "            sys.stdout.flush()\n",
    "\t\t# INSERT YOUR CODE AFTER ALL MINI_BATCHES HERE\n",
    "\t\t# MAKE SURE TO COMPUTE train_loss, train_accuracy, test_loss, test_accuracy\n",
    "    \n",
    "        exit()\n",
    "        print()\n",
    "        print('    Train Loss: {:.3f}    Train Acc.: {:.2f}%'.format(\n",
    "            train_loss,\n",
    "            100. * train_accuracy,\n",
    "        ))\n",
    "        print('    Test Loss:  {:.3f}    Test Acc.:  {:.2f}%'.format(\n",
    "            test_loss,\n",
    "            100. * test_accuracy,\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
